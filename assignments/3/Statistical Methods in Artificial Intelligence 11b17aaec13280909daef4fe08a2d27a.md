# Statistical Methods in Artificial Intelligence

Assignment 3

2022101104

## **Multi Layer Perceptron Classification**

### 2.1 Dataset Analysis and Preprocessing

![image.png](Statistical%20Methods%20in%20Artificial%20Intelligence%2011b17aaec13280909daef4fe08a2d27a/image.png)

![image.png](Statistical%20Methods%20in%20Artificial%20Intelligence%2011b17aaec13280909daef4fe08a2d27a/image%201.png)

![image.png](Statistical%20Methods%20in%20Artificial%20Intelligence%2011b17aaec13280909daef4fe08a2d27a/image%202.png)

| **Column Name**      | **Mean** | **Min** | **Max** | **Std** |
| -------------------- | -------- | ------- | ------- | ------- |
| fixed acidity        | 8.31     | 4.60    | 15.90   | 1.75    |
| volatile acidity     | 0.53     | 0.12    | 1.58    | 0.18    |
| citric acid          | 0.27     | 0.00    | 1.00    | 0.20    |
| residual sugar       | 2.53     | 0.90    | 15.50   | 1.36    |
| chlorides            | 0.09     | 0.01    | 0.61    | 0.05    |
| free sulfur dioxide  | 15.62    | 1.00    | 68.00   | 10.25   |
| total sulfur dioxide | 45.91    | 6.00    | 289.00  | 32.78   |
| density              | 1.00     | 0.99    | 1.00    | 0.00    |
| pH                   | 3.31     | 2.74    | 4.01    | 0.16    |
| sulphates            | 0.66     | 0.33    | 2.00    | 0.17    |
| alcohol              | 10.44    | 8.40    | 14.90   | 1.08    |
| quality              | 5.66     | 3.00    | 8.00    | 0.81    |
| Id                   | 804.97   | 0.00    | 1597.00 | 464.00  |

### 2.2 Model Building from Scratch

The core structure of the MLPClassifier includes:

- Initialization with customizable parameters
- Forward and backward propagation methods
- Training method with support for different optimization techniques
- Prediction method for making inferences
- Methods for saving and loading model parameters

### Activation Functions

The MLPClassifier implements four activation functions:

1. Sigmoid
2. Tanh (Hyperbolic Tangent)
3. ReLU (Rectified Linear Unit)
4. Linear

These functions are crucial for introducing non-linearity into the model, allowing it to learn complex patterns. The implementation allows for easy interchangeability of these functions, facilitating experimentation with different activation strategies for various layers of the network.

### Optimization Techniques

Three optimization techniques are implemented:

1. Stochastic Gradient Descent (SGD)
2. Batch Gradient Descent
3. Mini-Batch Gradient Descent

These optimizers dictate how the neural network updates its weights during training. The implementation allows users to choose the most suitable optimization strategy for their specific problem, balancing factors such as training speed, memory usage, and convergence characteristics.

Also implemented gradient checking which allows for validation of the computed gradients by comparing them with numerical approximations. This is essential for debugging and ensuring the correctness of the backpropagation implementation, particularly when dealing with complex network architectures.

### 2.3 Model Training & Hyperparameter Tuning using W&B

- https://wandb.ai/vcnk4v/wine-quality-classifier?nw=nwuservcnk4v : Initial run
- https://wandb.ai/vcnk4v/MLP-HyperparameterTuningSingle?nw=nwuservcnk4v : Hyper-parameter Tuning

```python
activation,batch_size,epochs,layers,learning_rate,optimizer,accuracy,epoch,f1,precision,recall,train_loss,val_loss
sigmoid,128,1000,"[16,32,16]",0.1,mini-batch,0.3779069767441861,6,0.1097046413502109,0.0755813953488372,0.2,1.805975332376304,1.8561346558605984
sigmoid,128,1000,"[16,32,16]",0.1,batch,0.4709302325581395,1000,0.1280632411067193,0.0941860465116279,0.2,1.1965957721018678,1.1535319180875705
sigmoid,128,1000,"[16,32,16]",0.1,sgd,0.5930232558139535,417,0.3251408946812257,0.3214346544382574,0.3301134014859505,0.9216701311769506,0.9321278116060902
sigmoid,128,1000,"[16,32,16]",0.01,mini-batch,0.377906976744186,6,0.1097046413502109,0.0755813953488372,0.2,1.805975332376304,1.8561346558605984
sigmoid,128,1000,"[16,32,16]",0.01,batch,0.4709302325581395,1000,0.1334601890943688,0.1941176470588235,0.2006077872744539,1.2259042922559331,1.1875665226930696
sigmoid,128,1000,"[16,32,16]",0.01,sgd,0.5988372093023255,1000,0.2936983176504135,0.3396483267158253,0.2984391933411541,0.9750651771406678,0.9745311698139908
sigmoid,128,1000,"[64,32]",0.1,mini-batch,0.4709302325581395,6,0.1280632411067193,0.0941860465116279,0.2,1.4473242166408349,1.398009967790305
sigmoid,128,1000,"[64,32]",0.1,batch,0.5174418604651163,1000,0.2101037136165942,0.195484363081617,0.2337321937321937,1.1533101521822204,1.116733356211315
sigmoid,128,1000,"[64,32]",0.1,sgd,0.6104651162790697,486,0.3570707767809217,0.347860646034162,0.3672308809563711,0.8957226001235905,0.9329304747299514
sigmoid,128,1000,"[64,32]",0.01,mini-batch,0.4709302325581395,6,0.1280632411067193,0.0941860465116279,0.2,1.4473242166408349,1.398009967790305
sigmoid,128,1000,"[64,32]",0.01,batch,0.4825581395348837,1000,0.1467479674796748,0.1826839826839827,0.2067616334283001,1.2008518298504651,1.1577283069444575
sigmoid,128,1000,"[64,32]",0.01,sgd,0.6337209302325582,1000,0.3671336115851907,0.3681777368481639,0.3690274286352718,0.9388664566471302,0.937920347963344
sigmoid,128,1000,[16],0.1,mini-batch,0.3546511627906977,6,0.1239086904285142,0.114074074074074,0.1963800904977375,1.8850730973050829,1.9583098816312117
sigmoid,128,1000,[16],0.1,batch,0.5755813953488372,1000,0.2473452206028354,0.2294372294372294,0.269363722697056,1.0323819640658405,1.0190934805816936
sigmoid,128,1000,[16],0.1,sgd,0.5988372093023255,113,0.3251947609309572,0.3337309495074237,0.3257181163063516,0.9033416329243208,0.9297976687442252
sigmoid,128,1000,[16],0.01,mini-batch,0.3546511627906977,6,0.1239086904285142,0.114074074074074,0.1963800904977375,1.8850730973050829,1.9583098816312117
sigmoid,128,1000,[16],0.01,batch,0.4244186046511627,1000,0.1832380952380952,0.1696010818120351,0.2003038936372269,1.2375119986769016,1.215651402629967
sigmoid,128,1000,[16],0.01,sgd,0.5988372093023255,979,0.3246627848781143,0.3326428370391219,0.3251103290318977,0.908628884972022,0.9284035013712936
sigmoid,128,100,"[16,32,16]",0.1,mini-batch,0.377906976744186,6,0.1097046413502109,0.0755813953488372,0.2,1.805975332376304,1.8561346558605984
sigmoid,128,100,"[16,32,16]",0.1,batch,0.4709302325581395,100,0.1280632411067193,0.0941860465116279,0.2,1.225667095174834,1.1872323314011075
sigmoid,128,100,"[16,32,16]",0.1,sgd,0.6104651162790697,100,0.2640189873417721,0.2475296039199673,0.2884330484330484,0.9838081311961396,0.9889849581332398
sigmoid,128,100,"[16,32,16]",0.01,mini-batch,0.377906976744186,6,0.1097046413502109,0.0755813953488372,0.2,1.805975332376304,1.8561346558605984
sigmoid,128,100,"[16,32,16]",0.01,batch,0.377906976744186,100,0.1097046413502109,0.0755813953488372,0.2,1.5556554455890943,1.577136163900291
sigmoid,128,100,"[16,32,16]",0.01,sgd,0.4709302325581395,100,0.1280632411067193,0.0941860465116279,0.2,1.1959626899810696,1.1525245901469343
sigmoid,128,100,"[64,32]",0.1,mini-batch,0.4709302325581395,6,0.1280632411067193,0.0941860465116279,0.2,1.4473242166408349,1.398009967790305
sigmoid,128,100,"[64,32]",0.1,batch,0.4825581395348837,100,0.1467479674796748,0.1826839826839827,0.2067616334283001,1.2007335461474644,1.1576283807827867
sigmoid,128,100,"[64,32]",0.1,sgd,0.6395348837209303,100,0.3729416169820559,0.3788260519967837,0.3727121389866488,0.940406556429741,0.9422371313937272
sigmoid,128,100,"[64,32]",0.01,mini-batch,0.4709302325581395,6,0.1280632411067193,0.0941860465116279,0.2,1.4473242166408349,1.398009967790305
sigmoid,128,100,"[64,32]",0.01,batch,0.4709302325581395,100,0.1280632411067193,0.0941860465116279,0.2,1.30504509888656,1.2599596002519176
sigmoid,128,100,"[64,32]",0.01,sgd,0.5465116279069767,100,0.231207729468599,0.2136432821364328,0.2521557454890788,1.1244072485416114,1.0942178328527576
sigmoid,128,100,[16],0.1,mini-batch,0.3546511627906977,6,0.1239086904285142,0.114074074074074,0.1963800904977375,1.8850730973050829,1.9583098816312117
sigmoid,128,100,[16],0.1,batch,0.4244186046511627,100,0.1832380952380952,0.1696010818120351,0.2003038936372269,1.2371201920238173,1.2150578633456792
sigmoid,128,100,[16],0.1,sgd,0.6104651162790697,100,0.332204155374887,0.3460881500686289,0.3312641751857438,0.9082223223526334,0.9298310805784552
sigmoid,128,100,[16],0.01,mini-batch,0.3546511627906977,6,0.1239086904285142,0.114074074074074,0.1963800904977375,1.8850730973050829,1.9583098816312117
sigmoid,128,100,[16],0.01,batch,0.3604651162790697,100,0.1064377682403433,0.0738095238095238,0.1907692307692307,1.6921546138595351,1.7440744225350702
sigmoid,128,100,[16],0.01,sgd,0.5872093023255814,100,0.2523966482665501,0.234090909090909,0.2749097815764482,1.008521700394914,0.9957918018144274
sigmoid,32,1000,"[16,32,16]",0.1,mini-batch,0.377906976744186,6,0.1097046413502109,0.0755813953488372,0.2,1.805975332376304,1.8561346558605984
sigmoid,32,1000,"[16,32,16]",0.1,batch,0.4709302325581395,1000,0.1280632411067193,0.0941860465116279,0.2,1.1965957721018678,1.1535319180875705
sigmoid,32,1000,"[16,32,16]",0.1,sgd,0.5930232558139535,417,0.3251408946812257,0.3214346544382574,0.3301134014859505,0.9216701311769506,0.9321278116060902
sigmoid,32,1000,"[16,32,16]",0.01,mini-batch,0.377906976744186,6,0.1097046413502109,0.0755813953488372,0.2,1.805975332376304,1.8561346558605984
sigmoid,32,1000,"[16,32,16]",0.01,batch,0.4709302325581395,1000,0.1334601890943688,0.1941176470588235,0.2006077872744539,1.2259042922559331,1.1875665226930696
sigmoid,32,1000,"[16,32,16]",0.01,sgd,0.5988372093023255,1000,0.2936983176504135,0.3396483267158253,0.2984391933411541,0.9750651771406678,0.9745311698139908
sigmoid,32,1000,"[64,32]",0.1,mini-batch,0.4709302325581395,6,0.1280632411067193,0.0941860465116279,0.2,1.4473242166408349,1.398009967790305
sigmoid,32,1000,"[64,32]",0.1,batch,0.5174418604651163,1000,0.2101037136165942,0.195484363081617,0.2337321937321937,1.1533101521822204,1.116733356211315
sigmoid,32,1000,"[64,32]",0.1,sgd,0.6104651162790697,486,0.3570707767809217,0.347860646034162,0.3672308809563711,0.8957226001235905,0.9329304747299514
sigmoid,32,1000,"[64,32]",0.01,mini-batch,0.4709302325581395,6,0.1280632411067193,0.0941860465116279,0.2,1.4473242166408349,1.398009967790305
sigmoid,32,1000,"[64,32]",0.01,batch,0.4825581395348837,1000,0.1467479674796748,0.1826839826839827,0.2067616334283001,1.2008518298504651,1.1577283069444575
sigmoid,32,1000,"[64,32]",0.01,sgd,0.6337209302325582,1000,0.3671336115851907,0.3681777368481639,0.3690274286352718,0.9388664566471302,0.937920347963344
sigmoid,32,1000,[16],0.1,mini-batch,0.3546511627906977,6,0.1239086904285142,0.114074074074074,0.1963800904977375,1.8850730973050829,1.9583098816312117
sigmoid,32,1000,[16],0.1,batch,0.5755813953488372,1000,0.2473452206028354,0.2294372294372294,0.269363722697056,1.0323819640658405,1.0190934805816936
sigmoid,32,1000,[16],0.1,sgd,0.5988372093023255,113,0.3251947609309572,0.3337309495074237,0.3257181163063516,0.9033416329243208,0.9297976687442252
sigmoid,32,1000,[16],0.01,mini-batch,0.3546511627906977,6,0.1239086904285142,0.114074074074074,0.1963800904977375,1.8850730973050829,1.9583098816312117
sigmoid,32,1000,[16],0.01,batch,0.4244186046511627,1000,0.1832380952380952,0.1696010818120351,0.2003038936372269,1.2375119986769016,1.215651402629967
sigmoid,32,1000,[16],0.01,sgd,0.5988372093023255,979,0.3246627848781143,0.3326428370391219,0.3251103290318977,0.908628884972022,0.9284035013712936
sigmoid,32,100,"[16,32,16]",0.1,mini-batch,0.377906976744186,6,0.1097046413502109,0.0755813953488372,0.2,1.805975332376304,1.8561346558605984
sigmoid,32,100,"[16,32,16]",0.1,batch,0.4709302325581395,100,0.1280632411067193,0.0941860465116279,0.2,1.225667095174834,1.1872323314011075
sigmoid,32,100,"[16,32,16]",0.1,sgd,0.6104651162790697,100,0.2640189873417721,0.2475296039199673,0.2884330484330484,0.9838081311961396,0.9889849581332398
sigmoid,32,100,"[16,32,16]",0.01,mini-batch,0.377906976744186,6,0.1097046413502109,0.0755813953488372,0.2,1.805975332376304,1.8561346558605984
sigmoid,32,100,"[16,32,16]",0.01,batch,0.377906976744186,100,0.1097046413502109,0.0755813953488372,0.2,1.5556554455890943,1.577136163900291
sigmoid,32,100,"[16,32,16]",0.01,sgd,0.4709302325581395,100,0.1280632411067193,0.0941860465116279,0.2,1.1959626899810696,1.1525245901469343
sigmoid,32,100,"[64,32]",0.1,mini-batch,0.4709302325581395,6,0.1280632411067193,0.0941860465116279,0.2,1.4473242166408349,1.398009967790305
sigmoid,32,100,"[64,32]",0.1,batch,0.4825581395348837,100,0.1467479674796748,0.1826839826839827,0.2067616334283001,1.2007335461474644,1.1576283807827867
sigmoid,32,100,"[64,32]",0.1,sgd,0.6395348837209303,100,0.3729416169820559,0.3788260519967837,0.3727121389866488,0.940406556429741,0.9422371313937272
sigmoid,32,100,"[64,32]",0.01,mini-batch,0.4709302325581395,6,0.1280632411067193,0.0941860465116279,0.2,1.4473242166408349,1.398009967790305
sigmoid,32,100,"[64,32]",0.01,batch,0.4709302325581395,100,0.1280632411067193,0.0941860465116279,0.2,1.30504509888656,1.2599596002519176
sigmoid,32,100,"[64,32]",0.01,sgd,0.5465116279069767,100,0.231207729468599,0.2136432821364328,0.2521557454890788,1.1244072485416114,1.0942178328527576
sigmoid,32,100,[16],0.1,mini-batch,0.3546511627906977,6,0.1239086904285142,0.114074074074074,0.1963800904977375,1.8850730973050829,1.9583098816312117
sigmoid,32,100,[16],0.1,batch,0.4244186046511627,100,0.1832380952380952,0.1696010818120351,0.2003038936372269,1.2371201920238173,1.2150578633456792
sigmoid,32,100,[16],0.1,sgd,0.6104651162790697,100,0.332204155374887,0.3460881500686289,0.3312641751857438,0.9082223223526334,0.9298310805784552
sigmoid,32,100,[16],0.01,mini-batch,0.3546511627906977,6,0.1239086904285142,0.114074074074074,0.1963800904977375,1.8850730973050829,1.9583098816312117
sigmoid,32,100,[16],0.01,batch,0.3604651162790697,100,0.1064377682403433,0.0738095238095238,0.1907692307692307,1.6921546138595351,1.7440744225350702
sigmoid,32,100,[16],0.01,sgd,0.5872093023255814,100,0.2523966482665501,0.234090909090909,0.2749097815764482,1.008521700394914,0.9957918018144274
tanh,128,1000,"[16,32,16]",0.1,mini-batch,0.2093023255813953,6,0.1132022827425126,0.1565077193668164,0.1361823361823362,1.8358048025228684,1.7953609122058454
tanh,128,1000,"[16,32,16]",0.1,batch,0.5872093023255814,642,0.3021280758903822,0.3357142857142857,0.3015809172671918,0.9048262750530616,0.952393931574222
tanh,128,1000,"[16,32,16]",0.1,sgd,0.5872093023255814,12,0.2895658904872841,0.3198255813953488,0.2947164962851237,0.9041091903863112,0.999583425562398
tanh,128,1000,"[16,32,16]",0.01,mini-batch,0.2093023255813953,6,0.1132022827425126,0.1565077193668164,0.1361823361823362,1.8358048025228684,1.7953609122058454
tanh,128,1000,"[16,32,16]",0.01,batch,0.5813953488372093,1000,0.2514619883040935,0.2345991561181434,0.2712250712250712,1.0081625488991408,1.0124562321311408
tanh,128,1000,"[16,32,16]",0.01,sgd,0.5872093023255814,56,0.3037116729424421,0.3543822843822844,0.3015809172671918,0.9009527933898271,0.9537361642982808
tanh,128,1000,"[64,32]",0.1,mini-batch,0.2558139534883721,6,0.1368548708974241,0.2077777777777778,0.2010762899651788,1.833130859132452,1.8314151456015413
tanh,128,1000,"[64,32]",0.1,batch,0.6162790697674418,289,0.3148705886577669,0.3472093023255814,0.3163577453773532,0.933521560049191,0.9430222249525144
tanh,128,1000,"[64,32]",0.1,sgd,0.5930232558139535,9,0.3253112565781029,0.3406189555125725,0.328111278699514,0.9256585307853384,0.9799598855986345
tanh,128,1000,"[64,32]",0.01,mini-batch,0.2558139534883721,6,0.1368548708974241,0.2077777777777778,0.2010762899651788,1.833130859132452,1.8314151456015413
tanh,128,1000,"[64,32]",0.01,batch,0.6046511627906976,1000,0.3108570916263223,0.3604200230149597,0.3102038992235071,0.97525059047179,0.9684013609115528
tanh,128,1000,"[64,32]",0.01,sgd,0.627906976744186,27,0.3329732723999446,0.365609243697479,0.3311993743366292,0.926684569895198,0.9423597294762076
tanh,128,1000,[16],0.1,mini-batch,0.1046511627906976,6,0.0719000287683519,0.1325845325845326,0.0720229782321285,2.013904859650122,2.0500029603083383
tanh,128,1000,[16],0.1,batch,0.6046511627906976,1000,0.3189039433771485,0.3409152491344272,0.317676107480029,0.914368705814274,0.9440768239802364
tanh,128,1000,[16],0.1,sgd,0.622093023255814,14,0.3508529334832064,0.3506493506493506,0.3535780124015418,0.9031542749910108,0.9533124547755164
tanh,128,1000,[16],0.01,mini-batch,0.1046511627906976,6,0.0719000287683519,0.1325845325845326,0.0720229782321285,2.013904859650122,2.0500029603083383
tanh,128,1000,[16],0.01,batch,0.5465116279069767,1000,0.236734693877551,0.2214185590131763,0.2551946818613485,1.0390697657010934,1.048974619049751
tanh,128,1000,[16],0.01,sgd,0.5930232558139535,89,0.3150735418427725,0.3380909090909091,0.3127378358750908,0.9091558258584896,0.9437354466967492
tanh,128,100,"[16,32,16]",0.1,mini-batch,0.2093023255813953,6,0.1132022827425126,0.1565077193668164,0.1361823361823362,1.8358048025228684,1.7953609122058454
tanh,128,100,"[16,32,16]",0.1,batch,0.5813953488372093,100,0.2514619883040935,0.2345991561181434,0.2712250712250712,1.0080116244048491,1.0122823992560828
tanh,128,100,"[16,32,16]",0.1,sgd,0.5872093023255814,12,0.2895658904872841,0.3198255813953488,0.2947164962851237,0.9041091903863112,0.999583425562398
tanh,128,100,"[16,32,16]",0.01,mini-batch,0.2093023255813953,6,0.1132022827425126,0.1565077193668164,0.1361823361823362,1.8358048025228684,1.7953609122058454
tanh,128,100,"[16,32,16]",0.01,batch,0.5465116279069767,100,0.2145196661007357,0.2703888098624941,0.2239539690520082,1.4098488543693883,1.3900622325382306
tanh,128,100,"[16,32,16]",0.01,sgd,0.5872093023255814,56,0.3037116729424421,0.3543822843822844,0.3015809172671918,0.9009527933898271,0.9537361642982808
tanh,128,100,"[64,32]",0.1,mini-batch,0.2558139534883721,6,0.1368548708974241,0.2077777777777778,0.2010762899651788,1.833130859132452,1.8314151456015413
tanh,128,100,"[64,32]",0.1,batch,0.6046511627906976,100,0.3108570916263223,0.3604200230149597,0.3102038992235071,0.9749876091453172,0.9680716586386496
tanh,128,100,"[64,32]",0.1,sgd,0.5930232558139535,9,0.3253112565781029,0.3406189555125725,0.328111278699514,0.9256585307853384,0.9799598855986345
tanh,128,100,"[64,32]",0.01,mini-batch,0.2558139534883721,6,0.1368548708974241,0.2077777777777778,0.2010762899651788,1.833130859132452,1.8314151456015413
tanh,128,100,"[64,32]",0.01,batch,0.5465116279069767,100,0.1981668118286259,0.18635477582846,0.2116492560937005,1.319506688174128,1.320631334205112
tanh,128,100,"[64,32]",0.01,sgd,0.627906976744186,27,0.3329732723999446,0.365609243697479,0.3311993743366292,0.926684569895198,0.9423597294762076
tanh,128,100,[16],0.1,mini-batch,0.1046511627906976,6,0.0719000287683519,0.1325845325845326,0.0720229782321285,2.013904859650122,2.0500029603083383
tanh,128,100,[16],0.1,batch,0.5406976744186046,100,0.2339142417119235,0.2186307519640852,0.2521177587844254,1.0386480443003814,1.048493182101209
tanh,128,100,[16],0.1,sgd,0.622093023255814,14,0.3508529334832064,0.3506493506493506,0.3535780124015418,0.9031542749910108,0.9533124547755164
tanh,128,100,[16],0.01,mini-batch,0.1046511627906976,6,0.0719000287683519,0.1325845325845326,0.0720229782321285,2.013904859650122,2.0500029603083383
tanh,128,100,[16],0.01,batch,0.3430232558139535,100,0.1402538902538902,0.1467661691542288,0.1350743906299462,1.6111093073243847,1.6488792911050858
tanh,128,100,[16],0.01,sgd,0.5930232558139535,89,0.3150735418427725,0.3380909090909091,0.3127378358750908,0.9091558258584896,0.9437354466967492
tanh,32,1000,"[16,32,16]",0.1,mini-batch,0.2093023255813953,6,0.1132022827425126,0.1565077193668164,0.1361823361823362,1.8358048025228684,1.7953609122058454
tanh,32,1000,"[16,32,16]",0.1,batch,0.5872093023255814,642,0.3021280758903822,0.3357142857142857,0.3015809172671918,0.9048262750530616,0.952393931574222
tanh,32,1000,"[16,32,16]",0.1,sgd,0.5872093023255814,12,0.2895658904872841,0.3198255813953488,0.2947164962851237,0.9041091903863112,0.999583425562398
tanh,32,1000,"[16,32,16]",0.01,mini-batch,0.2093023255813953,6,0.1132022827425126,0.1565077193668164,0.1361823361823362,1.8358048025228684,1.7953609122058454
tanh,32,1000,"[16,32,16]",0.01,batch,0.5813953488372093,1000,0.2514619883040935,0.2345991561181434,0.2712250712250712,1.0081625488991408,1.0124562321311408
tanh,32,1000,"[16,32,16]",0.01,sgd,0.5872093023255814,56,0.3037116729424421,0.3543822843822844,0.3015809172671918,0.9009527933898271,0.9537361642982808
tanh,32,1000,"[64,32]",0.1,mini-batch,0.2558139534883721,6,0.1368548708974241,0.2077777777777778,0.2010762899651788,1.833130859132452,1.8314151456015413
tanh,32,1000,"[64,32]",0.1,batch,0.6162790697674418,289,0.3148705886577669,0.3472093023255814,0.3163577453773532,0.933521560049191,0.9430222249525144
tanh,32,1000,"[64,32]",0.1,sgd,0.5930232558139535,9,0.3253112565781029,0.3406189555125725,0.328111278699514,0.9256585307853384,0.9799598855986345
tanh,32,1000,"[64,32]",0.01,mini-batch,0.2558139534883721,6,0.1368548708974241,0.2077777777777778,0.2010762899651788,1.833130859132452,1.8314151456015413
tanh,32,1000,"[64,32]",0.01,batch,0.6046511627906976,1000,0.3108570916263223,0.3604200230149597,0.3102038992235071,0.97525059047179,0.9684013609115528
tanh,32,1000,"[64,32]",0.01,sgd,0.627906976744186,27,0.3329732723999446,0.365609243697479,0.3311993743366292,0.926684569895198,0.9423597294762076
tanh,32,1000,[16],0.1,mini-batch,0.1046511627906976,6,0.0719000287683519,0.1325845325845326,0.0720229782321285,2.013904859650122,2.0500029603083383
tanh,32,1000,[16],0.1,batch,0.6046511627906976,1000,0.3189039433771485,0.3409152491344272,0.317676107480029,0.914368705814274,0.9440768239802364
tanh,32,1000,[16],0.1,sgd,0.622093023255814,14,0.3508529334832064,0.3506493506493506,0.3535780124015418,0.9031542749910108,0.9533124547755164
tanh,32,1000,[16],0.01,mini-batch,0.1046511627906976,6,0.0719000287683519,0.1325845325845326,0.0720229782321285,2.013904859650122,2.0500029603083383
tanh,32,1000,[16],0.01,batch,0.5465116279069767,1000,0.236734693877551,0.2214185590131763,0.2551946818613485,1.0390697657010934,1.048974619049751
tanh,32,1000,[16],0.01,sgd,0.5930232558139535,89,0.3150735418427725,0.3380909090909091,0.3127378358750908,0.9091558258584896,0.9437354466967492
tanh,32,100,"[16,32,16]",0.1,mini-batch,0.2093023255813953,6,0.1132022827425126,0.1565077193668164,0.1361823361823362,1.8358048025228684,1.7953609122058454
tanh,32,100,"[16,32,16]",0.1,batch,0.5813953488372093,100,0.2514619883040935,0.2345991561181434,0.2712250712250712,1.0080116244048491,1.0122823992560828
tanh,32,100,"[16,32,16]",0.1,sgd,0.5872093023255814,12,0.2895658904872841,0.3198255813953488,0.2947164962851237,0.9041091903863112,0.999583425562398
tanh,32,100,"[16,32,16]",0.01,mini-batch,0.2093023255813953,6,0.1132022827425126,0.1565077193668164,0.1361823361823362,1.8358048025228684,1.7953609122058454
tanh,32,100,"[16,32,16]",0.01,batch,0.5465116279069767,100,0.2145196661007357,0.2703888098624941,0.2239539690520082,1.4098488543693883,1.3900622325382306
tanh,32,100,"[16,32,16]",0.01,sgd,0.5872093023255814,56,0.3037116729424421,0.3543822843822844,0.3015809172671918,0.9009527933898271,0.9537361642982808
tanh,32,100,"[64,32]",0.1,mini-batch,0.2558139534883721,6,0.1368548708974241,0.2077777777777778,0.2010762899651788,1.833130859132452,1.8314151456015413
tanh,32,100,"[64,32]",0.1,batch,0.6046511627906976,100,0.3108570916263223,0.3604200230149597,0.3102038992235071,0.9749876091453172,0.9680716586386496
tanh,32,100,"[64,32]",0.1,sgd,0.5930232558139535,9,0.3253112565781029,0.3406189555125725,0.328111278699514,0.9256585307853384,0.9799598855986345
tanh,32,100,"[64,32]",0.01,mini-batch,0.2558139534883721,6,0.1368548708974241,0.2077777777777778,0.2010762899651788,1.833130859132452,1.8314151456015413
tanh,32,100,"[64,32]",0.01,batch,0.5465116279069767,100,0.1981668118286259,0.18635477582846,0.2116492560937005,1.319506688174128,1.320631334205112
tanh,32,100,"[64,32]",0.01,sgd,0.627906976744186,27,0.3329732723999446,0.365609243697479,0.3311993743366292,0.926684569895198,0.9423597294762076
tanh,32,100,[16],0.1,mini-batch,0.1046511627906976,6,0.0719000287683519,0.1325845325845326,0.0720229782321285,2.013904859650122,2.0500029603083383
tanh,32,100,[16],0.1,batch,0.5406976744186046,100,0.2339142417119235,0.2186307519640852,0.2521177587844254,1.0386480443003814,1.048493182101209
tanh,32,100,[16],0.1,sgd,0.622093023255814,14,0.3508529334832064,0.3506493506493506,0.3535780124015418,0.9031542749910108,0.9533124547755164
tanh,32,100,[16],0.01,mini-batch,0.1046511627906976,6,0.0719000287683519,0.1325845325845326,0.0720229782321285,2.013904859650122,2.0500029603083383
tanh,32,100,[16],0.01,batch,0.3430232558139535,100,0.1402538902538902,0.1467661691542288,0.1350743906299462,1.6111093073243847,1.6488792911050858
tanh,32,100,[16],0.01,sgd,0.5930232558139535,89,0.3150735418427725,0.3380909090909091,0.3127378358750908,0.9091558258584896,0.9437354466967492
relu,128,1000,"[16,32,16]",0.1,mini-batch,0.377906976744186,6,0.1120689655172413,0.0778443113772455,0.2,1.7210690961006387,1.6947137336703222
relu,128,1000,"[16,32,16]",0.1,batch,0.5813953488372093,228,0.3283187739463601,0.3187266553480475,0.3401195463940561,0.9170812264680528,1.0736617839229716
relu,128,1000,"[16,32,16]",0.1,sgd,0.6046511627906976,14,0.3409295352323838,0.421970062771608,0.3366962739511759,0.9193277175672432,1.1842838197675185
relu,128,1000,"[16,32,16]",0.01,mini-batch,0.377906976744186,6,0.1120689655172413,0.0778443113772455,0.2,1.7210690961006387,1.6947137336703222
relu,128,1000,"[16,32,16]",0.01,batch,0.5697674418604651,1000,0.3117520556609741,0.3179161579161579,0.3147667728059885,0.996329239983227,1.092422521441826
relu,128,1000,"[16,32,16]",0.01,sgd,0.5988372093023255,18,0.3180492709904474,0.3372100122100122,0.3194614825987375,0.9546682079003032,1.1154543597038429
relu,128,1000,"[64,32]",0.1,mini-batch,0.3662790697674418,6,0.1466394430311956,0.171192008581389,0.154388023015474,1.6276788803481252,1.610724930245105
relu,128,1000,"[64,32]",0.1,batch,0.6104651162790697,140,0.3396767380333044,0.3459899749373433,0.3399519579911737,0.9427126670802513,0.9924024949938168
relu,128,1000,"[64,32]",0.1,sgd,0.5348837209302325,6,0.2503531795367684,0.4417739130434782,0.270492151276465,0.946410195764517,1.1302558429991292
relu,128,1000,"[64,32]",0.01,mini-batch,0.3662790697674418,6,0.1466394430311956,0.171192008581389,0.154388023015474,1.6276788803481252,1.610724930245105
relu,128,1000,"[64,32]",0.01,batch,0.6337209302325582,1000,0.3545939627052812,0.3728647822765469,0.3510440757499581,0.966648597979515,0.995085068564052
relu,128,1000,"[64,32]",0.01,sgd,0.627906976744186,14,0.3481360465436899,0.3954591535017319,0.3423183062398748,0.9299800555595364,1.005525138200551
relu,128,1000,[16],0.1,mini-batch,0.3081395348837209,6,0.1427780213402806,0.1517608238038345,0.1880444295476975,1.9067090537968008,1.9155906056118597
relu,128,1000,[16],0.1,batch,0.5872093023255814,194,0.3394921638217733,0.3489898989898989,0.3369398357633651,0.9878526551659524,1.0090495791689202
relu,128,1000,[16],0.1,sgd,0.627906976744186,17,0.3464664664664664,0.3853785197722726,0.3441416680632367,0.9237279564099754,1.110352020421113
relu,128,1000,[16],0.01,mini-batch,0.3081395348837209,6,0.1427780213402806,0.1517608238038345,0.1880444295476975,1.9067090537968008,1.9155906056118597
relu,128,1000,[16],0.01,batch,0.5813953488372093,1000,0.3028049575994781,0.3799702000851426,0.2997195687391766,1.0717227592385656,1.060208426128128
relu,128,1000,[16],0.01,sgd,0.5930232558139535,19,0.3341479178098896,0.3463203463203463,0.3319367633093123,0.9698983242731046,1.0160658650188177
relu,128,100,"[16,32,16]",0.1,mini-batch,0.377906976744186,6,0.1120689655172413,0.0778443113772455,0.2,1.7210690961006387,1.6947137336703222
relu,128,100,"[16,32,16]",0.1,batch,0.5755813953488372,100,0.3139526873923683,0.3206095791001451,0.3172359086084576,0.9975166098894708,1.0928210151964353
relu,128,100,"[16,32,16]",0.1,sgd,0.6046511627906976,14,0.3409295352323838,0.421970062771608,0.3366962739511759,0.9193277175672432,1.1842838197675185
relu,128,100,"[16,32,16]",0.01,mini-batch,0.377906976744186,6,0.1120689655172413,0.0778443113772455,0.2,1.7210690961006387,1.6947137336703222
relu,128,100,"[16,32,16]",0.01,batch,0.377906976744186,100,0.1097046413502109,0.0755813953488372,0.2,1.603570807769994,1.5874237220500975
relu,128,100,"[16,32,16]",0.01,sgd,0.5988372093023255,18,0.3180492709904474,0.3372100122100122,0.3194614825987375,0.9546682079003032,1.1154543597038429
relu,128,100,"[64,32]",0.1,mini-batch,0.3662790697674418,6,0.1466394430311956,0.171192008581389,0.154388023015474,1.6276788803481252,1.610724930245105
relu,128,100,"[64,32]",0.1,batch,0.6337209302325582,100,0.3545939627052812,0.3728647822765469,0.3510440757499581,0.9668864029412916,0.9948973264700044
relu,128,100,"[64,32]",0.1,sgd,0.5348837209302325,6,0.2503531795367684,0.4417739130434782,0.270492151276465,0.946410195764517,1.1302558429991292
relu,128,100,"[64,32]",0.01,mini-batch,0.3662790697674418,6,0.1466394430311956,0.171192008581389,0.154388023015474,1.6276788803481252,1.610724930245105
relu,128,100,"[64,32]",0.01,batch,0.5116279069767442,100,0.1897339964411188,0.182516339869281,0.2251851851851852,1.2925473771914318,1.2626884076457932
relu,128,100,"[64,32]",0.01,sgd,0.627906976744186,14,0.3481360465436899,0.3954591535017319,0.3423183062398748,0.9299800555595364,1.005525138200551
relu,128,100,[16],0.1,mini-batch,0.3081395348837209,6,0.1427780213402806,0.1517608238038345,0.1880444295476975,1.9067090537968008,1.9155906056118597
relu,128,100,[16],0.1,batch,0.5813953488372093,100,0.3028049575994781,0.3799702000851426,0.2997195687391766,1.073162610365444,1.061540378121178
relu,128,100,[16],0.1,sgd,0.627906976744186,17,0.3464664664664664,0.3853785197722726,0.3441416680632367,0.9237279564099754,1.110352020421113
relu,128,100,[16],0.01,mini-batch,0.3081395348837209,6,0.1427780213402806,0.1517608238038345,0.1880444295476975,1.9067090537968008,1.9155906056118597
relu,128,100,[16],0.01,batch,0.3720930232558139,100,0.156190772328832,0.1835635769459299,0.1905293931437722,1.7406911220986625,1.7522587476933007
relu,128,100,[16],0.01,sgd,0.5930232558139535,19,0.3341479178098896,0.3463203463203463,0.3319367633093123,0.9698983242731046,1.0160658650188177
relu,32,1000,"[16,32,16]",0.1,mini-batch,0.377906976744186,6,0.1120689655172413,0.0778443113772455,0.2,1.7210690961006387,1.6947137336703222
relu,32,1000,"[16,32,16]",0.1,batch,0.5813953488372093,228,0.3283187739463601,0.3187266553480475,0.3401195463940561,0.9170812264680528,1.0736617839229716
relu,32,1000,"[16,32,16]",0.1,sgd,0.6046511627906976,14,0.3409295352323838,0.421970062771608,0.3366962739511759,0.9193277175672432,1.1842838197675185
relu,32,1000,"[16,32,16]",0.01,mini-batch,0.377906976744186,6,0.1120689655172413,0.0778443113772455,0.2,1.7210690961006387,1.6947137336703222
relu,32,1000,"[16,32,16]",0.01,batch,0.5697674418604651,1000,0.3117520556609741,0.3179161579161579,0.3147667728059885,0.996329239983227,1.092422521441826
relu,32,1000,"[16,32,16]",0.01,sgd,0.5988372093023255,18,0.3180492709904474,0.3372100122100122,0.3194614825987375,0.9546682079003032,1.1154543597038429
relu,32,1000,"[64,32]",0.1,mini-batch,0.3662790697674418,6,0.1466394430311956,0.171192008581389,0.154388023015474,1.6276788803481252,1.610724930245105
relu,32,1000,"[64,32]",0.1,batch,0.6104651162790697,140,0.3396767380333044,0.3459899749373433,0.3399519579911737,0.9427126670802513,0.9924024949938168
relu,32,1000,"[64,32]",0.1,sgd,0.5348837209302325,6,0.2503531795367684,0.4417739130434782,0.270492151276465,0.946410195764517,1.1302558429991292
relu,32,1000,"[64,32]",0.01,mini-batch,0.3662790697674418,6,0.1466394430311956,0.171192008581389,0.154388023015474,1.6276788803481252,1.610724930245105
relu,32,1000,"[64,32]",0.01,batch,0.6337209302325582,1000,0.3545939627052812,0.3728647822765469,0.3510440757499581,0.966648597979515,0.995085068564052
relu,32,1000,"[64,32]",0.01,sgd,0.627906976744186,14,0.3481360465436899,0.3954591535017319,0.3423183062398748,0.9299800555595364,1.005525138200551
relu,32,1000,[16],0.1,mini-batch,0.3081395348837209,6,0.1427780213402806,0.1517608238038345,0.1880444295476975,1.9067090537968008,1.9155906056118597
relu,32,1000,[16],0.1,batch,0.5872093023255814,194,0.3394921638217733,0.3489898989898989,0.3369398357633651,0.9878526551659524,1.0090495791689202
relu,32,1000,[16],0.1,sgd,0.627906976744186,17,0.3464664664664664,0.3853785197722726,0.3441416680632367,0.9237279564099754,1.110352020421113
relu,32,1000,[16],0.01,mini-batch,0.3081395348837209,6,0.1427780213402806,0.1517608238038345,0.1880444295476975,1.9067090537968008,1.9155906056118597
relu,32,1000,[16],0.01,batch,0.5813953488372093,1000,0.3028049575994781,0.3799702000851426,0.2997195687391766,1.0717227592385656,1.060208426128128
relu,32,1000,[16],0.01,sgd,0.5930232558139535,19,0.3341479178098896,0.3463203463203463,0.3319367633093123,0.9698983242731046,1.0160658650188177
relu,32,100,"[16,32,16]",0.1,mini-batch,0.377906976744186,6,0.1120689655172413,0.0778443113772455,0.2,1.7210690961006387,1.6947137336703222
relu,32,100,"[16,32,16]",0.1,batch,0.5755813953488372,100,0.3139526873923683,0.3206095791001451,0.3172359086084576,0.9975166098894708,1.0928210151964353
relu,32,100,"[16,32,16]",0.1,sgd,0.6046511627906976,14,0.3409295352323838,0.421970062771608,0.3366962739511759,0.9193277175672432,1.1842838197675185
relu,32,100,"[16,32,16]",0.01,mini-batch,0.377906976744186,6,0.1120689655172413,0.0778443113772455,0.2,1.7210690961006387,1.6947137336703222
relu,32,100,"[16,32,16]",0.01,batch,0.377906976744186,100,0.1097046413502109,0.0755813953488372,0.2,1.603570807769994,1.5874237220500975
relu,32,100,"[16,32,16]",0.01,sgd,0.5988372093023255,18,0.3180492709904474,0.3372100122100122,0.3194614825987375,0.9546682079003032,1.1154543597038429
relu,32,100,"[64,32]",0.1,mini-batch,0.3662790697674418,6,0.1466394430311956,0.171192008581389,0.154388023015474,1.6276788803481252,1.610724930245105
relu,32,100,"[64,32]",0.1,batch,0.6337209302325582,100,0.3545939627052812,0.3728647822765469,0.3510440757499581,0.9668864029412916,0.9948973264700044
relu,32,100,"[64,32]",0.01,batch,0.5116279069767442,100,0.1897339964411188,0.182516339869281,0.2251851851851852,1.2925473771914318,1.2626884076457932
relu,32,100,"[64,32]",0.1,sgd,0.5348837209302325,6,0.2503531795367684,0.4417739130434782,0.270492151276465,0.946410195764517,1.1302558429991292
relu,32,100,"[64,32]",0.01,mini-batch,0.3662790697674418,6,0.1466394430311956,0.171192008581389,0.154388023015474,1.6276788803481252,1.610724930245105
relu,32,100,"[64,32]",0.01,batch,0.5116279069767442,100,0.1897339964411188,0.182516339869281,0.2251851851851852,1.2925473771914318,1.2626884076457932
relu,32,100,"[64,32]",0.01,sgd,0.627906976744186,14,0.3481360465436899,0.3954591535017319,0.3423183062398748,0.9299800555595364,1.005525138200551
relu,32,100,[16],0.1,mini-batch,0.3081395348837209,6,0.1427780213402806,0.1517608238038345,0.1880444295476975,1.9067090537968008,1.9155906056118597
relu,32,100,[16],0.1,batch,0.5813953488372093,100,0.3028049575994781,0.3799702000851426,0.2997195687391766,1.073162610365444,1.061540378121178
relu,32,100,[16],0.1,sgd,0.627906976744186,17,0.3464664664664664,0.3853785197722726,0.3441416680632367,0.9237279564099754,1.110352020421113
relu,32,100,[16],0.01,mini-batch,0.3081395348837209,6,0.1427780213402806,0.1517608238038345,0.1880444295476975,1.9067090537968008,1.9155906056118597
relu,32,100,[16],0.01,batch,0.3720930232558139,100,0.156190772328832,0.1835635769459299,0.1905293931437722,1.7406911220986625,1.7522587476933007
relu,32,100,[16],0.01,sgd,0.5930232558139535,19,0.3341479178098896,0.3463203463203463,0.3319367633093123,0.9698983242731046,1.0160658650188177
```

**Best Model Parameters:**

```python
{
  "input_size": 11,
  "output_size": 6,
  "hidden_layers": [64, 32],
  "activation": "sigmoid",
  "optimizer": "sgd",
  "learning_rate": 0.1,
  "batch_size": 128,
  "epochs": 100,
  "early_stopping": true,
  "patience": 5,
  "wandb_log": true
}
```

### 2.4 Evaluating Single-label Classification Model

https://wandb.ai/vcnk4v/MLP-single-test?nw=nwuservcnk4v

```python
Test accuracy of best model: 0.672514619883041
Test F1 score of best model: 0.39489560405286184
Test precision of best model: 0.40840620031796504
Test recall of best model: 0.39064117881322186
```

### 2.5 Analyzing Hyperparameters Effects

https://wandb.ai/vcnk4v/MLP-Single-Experiment?nw=nwuservcnk4v

![image.png](Statistical%20Methods%20in%20Artificial%20Intelligence%2011b17aaec13280909daef4fe08a2d27a/image%203.png)

### 1. **ReLU (blue)**

- **Convergence Speed**: ReLU demonstrates quick convergence. The loss drops rapidly in the early epochs, indicating efficient learning early in the training process.
- **Stability**: The loss stabilizes well around a low value within the first 10 epochs, suggesting that ReLU performs effectively in terms of both speed and stability.

### 2. **Tanh (orange)**

- **Convergence Speed**: Tanh also shows quick convergence, similar to ReLU, but it converges slightly faster than ReLU.
- **Final Loss**: It reaches the lowest loss among all the activation functions, making it potentially the best choice in terms of final model accuracy.

### 3. **Sigmoid (green)**

- **Convergence Speed**: Sigmoid starts with a high initial loss and converges more slowly than both ReLU and Tanh.
- **Final Loss**: The model continues to decrease in loss even after 100 epochs, but it does so at a much slower rate, indicating a sluggish learning process. Sigmoid often struggles with vanishing gradients in deep layers, which may explain its poor convergence speed here.

### 4. **Linear (red)**

- **Convergence Speed**: Linear activation has a steady decline in loss, but it converges quite slowly compared to Tanh and ReLU.
- **Final Loss**: Although it stabilizes over time, it doesn’t reach as low a final loss as Tanh and ReLU. Since Linear activation lacks non-linearity, it limits the model’s ability to learn complex patterns, which could explain the higher loss.

### General Observations:

- **Tanh and ReLU** show the best performance in terms of convergence speed and final loss.
- **Sigmoid** struggles with slow convergence and a high initial loss, possibly due to gradient issues.
- **Linear** converges slowly and reaches a relatively higher loss, indicating it’s less suitable for this classification task compared to the non-linear activations.

![image.png](Statistical%20Methods%20in%20Artificial%20Intelligence%2011b17aaec13280909daef4fe08a2d27a/image%204.png)

### 1. **Learning Rate = 0.001 (Blue)**

- **Convergence Speed**: This learning rate is quite small, and the model converges very slowly. Even after 100 epochs, the loss remains relatively high, indicating that the model is taking much longer to learn and adjust the weights effectively.
- **Final Loss**: Although the model is still decreasing in loss, it is far from converging within the given number of epochs, suggesting that this learning rate is too small for efficient training.

### 2. **Learning Rate = 0.01 (Orange)**

- **Convergence Speed**: With a higher learning rate, the model converges faster than with 0.001. However, the convergence is still somewhat slow compared to larger learning rates.
- **Final Loss**: The loss continues to decrease but at a slow rate, and the model hasn't fully converged within 100 epochs. This suggests that while this learning rate is better, it may still be too conservative for rapid convergence.

### 3. **Learning Rate = 0.1 (Green)**

- **Convergence Speed**: This learning rate shows much faster convergence. The loss decreases sharply in the early epochs, demonstrating efficient learning. The model stabilizes much faster compared to the smaller learning rates.
- **Final Loss**: The model reaches a significantly lower loss compared to 0.001 and 0.01, making it a good balance between speed and stability. It shows that 0.1 allows the model to learn effectively and converge within a reasonable number of epochs.

### 4. **Learning Rate = 0.5 (Red)**

- **Convergence Speed**: The learning rate of 0.5 results in the fastest convergence of all the rates. The loss drops quickly and stabilizes within just a few epochs.
- **Final Loss**: However, there is a risk that the learning rate is too high, which may cause the model to converge to a suboptimal point (as seen in the flattening loss). The model converges quickly, but it doesn’t achieve a significantly lower loss compared to the 0.1 learning rate. This indicates potential overshooting, where the model skips over the optimal solution.

### General Observations:

- **Low learning rates (0.001, 0.01)** result in slow convergence and higher final losses. The model takes too long to adjust its weights properly.
- **Moderate learning rates (0.1)** strike a good balance between convergence speed and final accuracy, allowing the model to converge quickly without overshooting.
- **High learning rates (0.5)** result in rapid convergence, but may cause the model to settle on suboptimal solutions due to overshooting.

![image.png](Statistical%20Methods%20in%20Artificial%20Intelligence%2011b17aaec13280909daef4fe08a2d27a/image%205.png)

![image.png](Statistical%20Methods%20in%20Artificial%20Intelligence%2011b17aaec13280909daef4fe08a2d27a/image%206.png)

### 1. **Batch Size = 16 (Blue)**

- **Convergence Speed**: The model with batch size 16 converges relatively quickly compared to the other batch sizes. The loss decreases rapidly, particularly in the early epochs, indicating efficient weight updates.
- **Final Loss**: By the end of the 100 epochs, the model reaches the lowest loss among all batch sizes, suggesting that a smaller batch size allows for better generalization and performance in this case.

### 2. **Batch Size = 32 (Orange)**

- **Convergence Speed**: Batch size 32 also converges efficiently, though slightly slower than batch size 16. The loss decreases at a steady pace, especially in the first half of the epochs.
- **Final Loss**: The model with batch size 32 ends up with a low loss value, but slightly higher than batch size 16. It is still an effective choice and shows good performance.

### 3. **Batch Size = 64 (Green)**

- **Convergence Speed**: The model with batch size 64 converges more slowly than batch sizes 16 and 32. The loss starts decreasing gradually but not as rapidly as the smaller batch sizes.
- **Final Loss**: After 100 epochs, the model reaches a higher final loss compared to smaller batch sizes, indicating that larger batch sizes may hinder effective weight updates during training.

### 4. **Batch Size = 128 (Red)**

- **Convergence Speed**: Batch size 128 converges the slowest out of all the options. The loss decreases very slowly over time, and it does not stabilize even by the end of 100 epochs.
- **Final Loss**: The final loss is higher than the other batch sizes, suggesting that this large batch size results in inefficient learning. With fewer weight updates per epoch, the model struggles to optimize properly.

### General Observations:

- **Smaller batch sizes (16 and 32)** result in faster convergence and better final loss values. These smaller batch sizes allow for more frequent weight updates, which help the model learn more effectively, albeit at the cost of higher computational expense per epoch.
- **Larger batch sizes (64 and 128)** result in slower convergence and higher final losses. Larger batch sizes may provide smoother gradient updates but can lead to slower learning and less effective optimization.

Note: Best was SGD so ran with best params and mini-batch to see effect

### 2.6 Multi-Label Classification

- https://wandb.ai/vcnk4v/multi-label-advertisement-classification/runs/fzy9mqjw?nw=nwuservcnk4v : Single Run
- https://wandb.ai/vcnk4v/MLP-Hyperparameter-Tuning-Multi_Label?nw=nwuservcnk4v : Hyper-parameter Tuning

The MLP class for multi-label classification closely resembles its single-label counterpart, with two key differences (used inheritance):

1. Output Layer Activation: Instead of softmax, multi-label classification employs sigmoid activation in the final layer.
2. Loss Function: Binary cross-entropy (BCE) replaces the standard cross-entropy loss used in single-label classification.

These adjustments allow the model to handle multiple labels per instance, as opposed to assigning a single class to each input.

**Hyperparameter Tuning**

```python
activation,batch_size,epochs,layers,learning_rate,optimizer,accuracy,epoch,f1,hamming_loss,precision,recall,train_loss,val_loss
sigmoid,128,1000,"[16,32,16]",0.1,mini-batch,0.5037499999999999,11,0.243980372018715,0.49625,0.6625,0.5,5.888870425837174,5.923496267308539
sigmoid,128,1000,"[16,32,16]",0.1,batch,0.67875,1000,0.0,0.32125,1.0,0.0,5.170251484054205,5.029896345793019
sigmoid,128,1000,"[16,32,16]",0.1,sgd,0.6837500000000001,416,0.1348901098901098,0.31625,0.8255602240896358,0.1036122311827957,5.005225853459266,4.884246403458142
sigmoid,128,1000,"[16,32,16]",0.01,mini-batch,0.5037499999999999,11,0.243980372018715,0.49625,0.6625,0.5,5.888870425837174,5.923496267308539
sigmoid,128,1000,"[16,32,16]",0.01,batch,0.67875,1000,0.0,0.32125,1.0,0.0,5.1712962904263735,5.033839601770805
sigmoid,128,1000,"[16,32,16]",0.01,sgd,0.67875,1000,0.0,0.32125,1.0,0.0,5.146783736165652,5.0071340517787215
sigmoid,128,1000,"[64,32]",0.1,mini-batch,0.4925,11,0.2759792287610209,0.5075,0.5863815789473685,0.5263157894736842,5.9410769204909295,5.954642183824445
sigmoid,128,1000,"[64,32]",0.1,batch,0.67875,1000,0.0,0.32125,1.0,0.0,5.151223743921614,5.016463022227599
sigmoid,128,1000,"[64,32]",0.1,sgd,0.6837500000000001,61,0.2235324947589098,0.31625,0.7589285714285714,0.0787550403225806,5.063307406459854,4.920676167397713
sigmoid,128,1000,"[64,32]",0.01,mini-batch,0.4925000000000001,11,0.2759792287610209,0.5075,0.5863815789473685,0.5263157894736842,5.9410769204909295,5.954642183824445
sigmoid,128,1000,"[64,32]",0.01,batch,0.67875,1000,0.0,0.32125,1.0,0.0,5.168423071409013,5.028235162142517
sigmoid,128,1000,"[64,32]",0.01,sgd,0.69,501,0.0958188153310104,0.31,0.902941176470588,0.0708165322580645,5.062016462203757,4.917218842705181
sigmoid,128,1000,[16],0.1,mini-batch,0.44375,11,0.3742340492649977,0.55625,0.4092553474641889,0.6454861111111111,5.933271403955896,6.008084031068226
sigmoid,128,1000,[16],0.1,batch,0.67625,1000,0.125,0.32375,0.875,0.0,5.09336601917184,4.983648547648481
sigmoid,128,1000,[16],0.1,sgd,0.6737500000000001,36,0.3476658476658476,0.32625,0.522184065934066,0.0751092069892473,5.045112281525272,4.952732635148465
sigmoid,128,1000,[16],0.01,mini-batch,0.44375,11,0.3742340492649977,0.55625,0.4092553474641889,0.6454861111111111,5.933271403955896,6.008084031068226
sigmoid,128,1000,[16],0.01,batch,0.67875,1000,0.0,0.32125,1.0,0.0,5.186378340602914,5.071158971719431
sigmoid,128,1000,[16],0.01,sgd,0.6799999999999999,269,0.2181087551299589,0.32,0.6666666666666666,0.0671706989247311,5.048468757971758,4.949673411868829
sigmoid,128,100,"[16,32,16]",0.1,mini-batch,0.5037499999999999,11,0.243980372018715,0.49625,0.6625,0.5,5.888870425837174,5.923496267308539
sigmoid,128,100,"[16,32,16]",0.1,batch,0.67875,100,0.0,0.32125,1.0,0.0,5.17125936035514,5.033595376047859
sigmoid,128,100,"[16,32,16]",0.1,sgd,0.67875,100,0.0,0.32125,1.0,0.0,5.148987941259363,5.0062636664915
sigmoid,128,100,"[16,32,16]",0.01,mini-batch,0.5037499999999999,11,0.243980372018715,0.49625,0.6625,0.5,5.888870425837174,5.923496267308539
sigmoid,128,100,"[16,32,16]",0.01,batch,0.5662499999999999,100,0.1290264853256979,0.43375,0.7758333333333334,0.2631578947368421,5.493845518623438,5.47102490481849
sigmoid,128,100,"[16,32,16]",0.01,sgd,0.67875,100,0.0,0.32125,1.0,0.0,5.170314690090618,5.029773639784069
sigmoid,128,100,"[64,32]",0.1,mini-batch,0.4925,11,0.2759792287610209,0.5075,0.5863815789473685,0.5263157894736842,5.9410769204909295,5.954642183824445
sigmoid,128,100,"[64,32]",0.1,batch,0.67875,100,0.0,0.32125,1.0,0.0,5.168424432138616,5.028222647220164
sigmoid,128,100,"[64,32]",0.1,sgd,0.6837500000000001,61,0.2235324947589098,0.31625,0.7589285714285714,0.0787550403225806,5.063307406459854,4.920676167397713
sigmoid,128,100,"[64,32]",0.01,mini-batch,0.4925,11,0.2759792287610209,0.5075,0.5863815789473685,0.5263157894736842,5.9410769204909295,5.954642183824445
sigmoid,128,100,"[64,32]",0.01,batch,0.58125,100,0.1168526130358191,0.41875,0.8262499999999999,0.25,5.353998631073989,5.282706592124439
sigmoid,128,100,"[64,32]",0.01,sgd,0.67875,100,0.0,0.32125,1.0,0.0,5.151458375276373,5.016338905692364
sigmoid,128,100,[16],0.1,mini-batch,0.44375,11,0.3742340492649977,0.55625,0.4092553474641889,0.6454861111111111,5.933271403955896,6.008084031068226
sigmoid,128,100,[16],0.1,batch,0.67875,100,0.0,0.32125,1.0,0.0,5.1862359833677765,5.070568032752126
sigmoid,128,100,[16],0.1,sgd,0.6737500000000001,36,0.3476658476658476,0.32625,0.522184065934066,0.0751092069892473,5.045112281525272,4.952732635148465
sigmoid,128,100,[16],0.01,mini-batch,0.44375,11,0.3742340492649977,0.55625,0.4092553474641889,0.6454861111111111,5.933271403955896,6.008084031068226
sigmoid,128,100,[16],0.01,batch,0.515,100,0.2900441786104087,0.485,0.484838036421247,0.4318213926062785,5.606232555053155,5.628967993391033
sigmoid,128,100,[16],0.01,sgd,0.67625,100,0.125,0.32375,0.875,0.0,5.09339142723737,4.983333380874243
sigmoid,32,1000,"[16,32,16]",0.1,mini-batch,0.5037499999999999,11,0.243980372018715,0.49625,0.6625,0.5,5.888870425837174,5.923496267308539
sigmoid,32,1000,"[16,32,16]",0.1,batch,0.67875,1000,0.0,0.32125,1.0,0.0,5.170251484054205,5.029896345793019
sigmoid,32,1000,"[16,32,16]",0.1,sgd,0.6837500000000001,416,0.1348901098901098,0.31625,0.8255602240896358,0.1036122311827957,5.005225853459266,4.884246403458142
sigmoid,32,1000,"[16,32,16]",0.01,mini-batch,0.5037499999999999,11,0.243980372018715,0.49625,0.6625,0.5,5.888870425837174,5.923496267308539
sigmoid,32,1000,"[16,32,16]",0.01,batch,0.67875,1000,0.0,0.32125,1.0,0.0,5.1712962904263735,5.033839601770805
sigmoid,32,1000,"[16,32,16]",0.01,sgd,0.67875,1000,0.0,0.32125,1.0,0.0,5.146783736165652,5.0071340517787215
sigmoid,32,1000,"[64,32]",0.1,mini-batch,0.4925,11,0.2759792287610209,0.5075,0.5863815789473685,0.5263157894736842,5.9410769204909295,5.954642183824445
sigmoid,32,1000,"[64,32]",0.1,batch,0.67875,1000,0.0,0.32125,1.0,0.0,5.151223743921614,5.016463022227599
sigmoid,32,1000,"[64,32]",0.1,sgd,0.6837500000000001,61,0.2235324947589098,0.31625,0.7589285714285714,0.0787550403225806,5.063307406459854,4.920676167397713
sigmoid,32,1000,"[64,32]",0.01,mini-batch,0.4925,11,0.2759792287610209,0.5075,0.5863815789473685,0.5263157894736842,5.9410769204909295,5.954642183824445
sigmoid,32,1000,"[64,32]",0.01,batch,0.67875,1000,0.0,0.32125,1.0,0.0,5.168423071409013,5.028235162142517
sigmoid,32,1000,"[64,32]",0.01,sgd,0.69,501,0.0958188153310104,0.31,0.902941176470588,0.0708165322580645,5.062016462203757,4.917218842705181
sigmoid,32,1000,[16],0.1,mini-batch,0.44375,11,0.3742340492649977,0.55625,0.4092553474641889,0.6454861111111111,5.933271403955896,6.008084031068226
sigmoid,32,1000,[16],0.1,batch,0.67625,1000,0.125,0.32375,0.875,0.0,5.09336601917184,4.983648547648481
sigmoid,32,1000,[16],0.1,sgd,0.6737500000000001,36,0.3476658476658476,0.32625,0.522184065934066,0.0751092069892473,5.045112281525272,4.952732635148465
sigmoid,32,1000,[16],0.01,mini-batch,0.44375,11,0.3742340492649977,0.55625,0.4092553474641889,0.6454861111111111,5.933271403955896,6.008084031068226
sigmoid,32,1000,[16],0.01,batch,0.67875,1000,0.0,0.32125,1.0,0.0,5.186378340602914,5.071158971719431
sigmoid,32,1000,[16],0.01,sgd,0.6799999999999999,269,0.2181087551299589,0.32,0.6666666666666666,0.0671706989247311,5.048468757971758,4.949673411868829
sigmoid,32,100,"[16,32,16]",0.1,mini-batch,0.5037499999999999,11,0.243980372018715,0.49625,0.6625,0.5,5.888870425837174,5.923496267308539
sigmoid,32,100,"[16,32,16]",0.1,batch,0.67875,100,0.0,0.32125,1.0,0.0,5.17125936035514,5.033595376047859
sigmoid,32,100,"[16,32,16]",0.1,sgd,0.67875,100,0.0,0.32125,1.0,0.0,5.148987941259363,5.0062636664915
sigmoid,32,100,"[16,32,16]",0.01,mini-batch,0.5037499999999999,11,0.243980372018715,0.49625,0.6625,0.5,5.888870425837174,5.923496267308539
sigmoid,32,100,"[16,32,16]",0.01,batch,0.5662499999999999,100,0.1290264853256979,0.43375,0.7758333333333334,0.2631578947368421,5.493845518623438,5.47102490481849
sigmoid,32,100,"[16,32,16]",0.01,sgd,0.67875,100,0.0,0.32125,1.0,0.0,5.170314690090618,5.029773639784069
sigmoid,32,100,"[64,32]",0.1,mini-batch,0.4925,11,0.2759792287610209,0.5075,0.5863815789473685,0.5263157894736842,5.9410769204909295,5.954642183824445
sigmoid,32,100,"[64,32]",0.1,batch,0.67875,100,0.0,0.32125,1.0,0.0,5.168424432138616,5.028222647220164
sigmoid,32,100,"[64,32]",0.1,sgd,0.6837500000000001,61,0.2235324947589098,0.31625,0.7589285714285714,0.0787550403225806,5.063307406459854,4.920676167397713
sigmoid,32,100,"[64,32]",0.01,mini-batch,0.4925,11,0.2759792287610209,0.5075,0.5863815789473685,0.5263157894736842,5.9410769204909295,5.954642183824445
sigmoid,32,100,"[64,32]",0.01,batch,0.58125,100,0.1168526130358191,0.41875,0.8262499999999999,0.25,5.353998631073989,5.282706592124439
sigmoid,32,100,"[64,32]",0.01,sgd,0.67875,100,0.0,0.32125,1.0,0.0,5.151458375276373,5.016338905692364
sigmoid,32,100,[16],0.1,mini-batch,0.44375,11,0.3742340492649977,0.55625,0.4092553474641889,0.6454861111111111,5.933271403955896,6.008084031068226
sigmoid,32,100,[16],0.1,batch,0.67875,100,0.0,0.32125,1.0,0.0,5.1862359833677765,5.070568032752126
sigmoid,32,100,[16],0.1,sgd,0.6737500000000001,36,0.3476658476658476,0.32625,0.522184065934066,0.0751092069892473,5.045112281525272,4.952732635148465
sigmoid,32,100,[16],0.01,mini-batch,0.44375,11,0.3742340492649977,0.55625,0.4092553474641889,0.6454861111111111,5.933271403955896,6.008084031068226
sigmoid,32,100,[16],0.01,batch,0.515,100,0.2900441786104087,0.485,0.484838036421247,0.4318213926062785,5.606232555053155,5.628967993391033
sigmoid,32,100,[16],0.01,sgd,0.67625,100,0.125,0.32375,0.875,0.0,5.09339142723737,4.983333380874243
tanh,128,1000,"[16,32,16]",0.1,mini-batch,0.4975,11,0.3882232176454856,0.5025,0.320990905085276,0.4985243922484104,5.788075795536461,5.681842188719091
tanh,128,1000,"[16,32,16]",0.1,batch,0.6799999999999999,172,0.3438479623824451,0.32,0.5897435897435898,0.0638534199522102,5.037790547000711,4.965190893255575
tanh,128,1000,"[16,32,16]",0.1,sgd,0.655,12,0.2466156867461008,0.345,0.4114911906791702,0.1902675075642405,4.99329483660571,5.041623525928673
tanh,128,1000,"[16,32,16]",0.01,mini-batch,0.4975,11,0.3882232176454856,0.5025,0.320990905085276,0.4985243922484104,5.788075795536461,5.681842188719091
tanh,128,1000,"[16,32,16]",0.01,batch,0.6837500000000001,1000,0.3292682926829268,0.31625,0.6736111111111112,0.0514112903225806,5.061945989418579,4.971529394616471
tanh,128,1000,"[16,32,16]",0.01,sgd,0.67625,26,0.3835169973930951,0.32375,0.4047619047619047,0.0955115092278186,5.024568276484556,4.967662797972688
tanh,128,1000,"[64,32]",0.1,mini-batch,0.5237499999999999,11,0.3884069767385712,0.47625,0.3341789217888725,0.4736946599050863,5.713691851698535,5.645412526585732
tanh,128,1000,"[64,32]",0.1,batch,0.6837500000000001,76,0.2305274382480265,0.31625,0.6961032388663968,0.0779084845950264,5.043916536201342,4.942256215049856
tanh,128,1000,"[64,32]",0.1,sgd,0.64375,15,0.2818757706658882,0.35625,0.4156542076139263,0.226621670763193,4.904944962498956,5.096925081134924
tanh,128,1000,"[64,32]",0.01,mini-batch,0.5237499999999999,11,0.3884069767385712,0.47625,0.3341789217888725,0.4736946599050863,5.713691851698535,5.645412526585732
tanh,128,1000,"[64,32]",0.01,batch,0.685,674,0.1055274382480264,0.315,0.8211032388663968,0.0779084845950264,5.046886498205977,4.941871518643393
tanh,128,1000,"[64,32]",0.01,sgd,0.6799999999999999,16,0.1449030168449601,0.32,0.593859649122807,0.1062576954357483,5.0348010328509485,4.9555522580031575
tanh,128,1000,[16],0.1,mini-batch,0.4975,11,0.3748163729286347,0.5025,0.3139447152170533,0.4703803111510592,5.946434882125978,5.963450127849592
tanh,128,1000,[16],0.1,batch,0.6799999999999999,221,0.130998334682207,0.32,0.6977591036414565,0.0914604185854685,5.036800298386169,4.95855914564903
tanh,128,1000,[16],0.1,sgd,0.66875,12,0.2043827226621026,0.33125,0.4458333333333333,0.1465118436310574,5.026914633930776,5.024334924424032
tanh,128,1000,[16],0.01,mini-batch,0.4975,11,0.3748163729286347,0.5025,0.3139447152170533,0.4703803111510591,5.946434882125978,5.963450127849592
tanh,128,1000,[16],0.01,batch,0.67625,1000,0.1071149379076208,0.32375,0.7335858585858586,0.0676865358598636,5.069924898441096,4.981831369543553
tanh,128,1000,[16],0.01,sgd,0.67875,31,0.388969178538144,0.32125,0.5426587301587301,0.0992723464126265,5.027708425481517,4.961994340083397
tanh,128,100,"[16,32,16]",0.1,mini-batch,0.4975,11,0.3882232176454856,0.5025,0.320990905085276,0.4985243922484104,5.788075795536461,5.681842188719091
tanh,128,100,"[16,32,16]",0.1,batch,0.6837500000000001,100,0.3292682926829268,0.31625,0.6736111111111112,0.0514112903225806,5.0620166212627655,4.97128372363239
tanh,128,100,"[16,32,16]",0.1,sgd,0.655,12,0.2466156867461008,0.345,0.4114911906791702,0.1902675075642405,4.99329483660571,5.041623525928673
tanh,128,100,"[16,32,16]",0.01,mini-batch,0.4975,11,0.3882232176454856,0.5025,0.320990905085276,0.4985243922484104,5.788075795536461,5.681842188719091
tanh,128,100,"[16,32,16]",0.01,batch,0.6425000000000001,100,0.2111041457729565,0.3575,0.3537425889328063,0.1694257419058927,5.3259440324112814,5.201966653451157
tanh,128,100,"[16,32,16]",0.01,sgd,0.67625,26,0.3835169973930951,0.32375,0.4047619047619047,0.0955115092278186,5.024568276484556,4.967662797972688
tanh,128,100,"[64,32]",0.1,mini-batch,0.5237499999999999,11,0.3884069767385712,0.47625,0.3341789217888725,0.4736946599050863,5.713691851698535,5.645412526585732
tanh,128,100,"[64,32]",0.1,batch,0.6837500000000001,76,0.2305274382480265,0.31625,0.6961032388663968,0.0779084845950264,5.043916536201342,4.942256215049856
tanh,128,100,"[64,32]",0.1,sgd,0.64375,15,0.2818757706658882,0.35625,0.4156542076139263,0.226621670763193,4.904944962498956,5.096925081134924
tanh,128,100,"[64,32]",0.01,mini-batch,0.5237499999999999,11,0.3884069767385712,0.47625,0.3341789217888725,0.4736946599050863,5.713691851698535,5.645412526585732
tanh,128,100,"[64,32]",0.01,batch,0.67875,100,0.2103985962179239,0.32125,0.5021008403361344,0.1466454662179906,5.21764164620472,5.115372143238818
tanh,128,100,"[64,32]",0.01,sgd,0.6799999999999999,16,0.1449030168449601,0.32,0.593859649122807,0.1062576954357483,5.0348010328509485,4.9555522580031575
tanh,128,100,[16],0.1,mini-batch,0.4975,11,0.3748163729286347,0.5025,0.3139447152170533,0.4703803111510591,5.946434882125978,5.963450127849592
tanh,128,100,[16],0.1,batch,0.67625,100,0.1071149379076208,0.32375,0.7335858585858586,0.0676865358598636,5.069896857066088,4.98158258265437
tanh,128,100,[16],0.1,sgd,0.66875,12,0.2043827226621026,0.33125,0.4458333333333333,0.1465118436310574,5.026914633930776,5.024334924424032
tanh,128,100,[16],0.01,mini-batch,0.4975,11,0.3748163729286347,0.5025,0.3139447152170533,0.4703803111510591,5.946434882125978,5.963450127849592
tanh,128,100,[16],0.01,batch,0.55625,100,0.3350190881699572,0.44375,0.3294914580440896,0.3512490966604525,5.586321893978202,5.578541330346592
tanh,128,100,[16],0.01,sgd,0.67875,31,0.388969178538144,0.32125,0.5426587301587301,0.0992723464126265,5.027708425481517,4.961994340083397
tanh,32,1000,"[16,32,16]",0.1,mini-batch,0.4975,11,0.3882232176454856,0.5025,0.320990905085276,0.4985243922484104,5.788075795536461,5.681842188719091
tanh,32,1000,"[16,32,16]",0.1,batch,0.6799999999999999,172,0.3438479623824451,0.32,0.5897435897435898,0.0638534199522102,5.037790547000711,4.965190893255575
tanh,32,1000,"[16,32,16]",0.1,sgd,0.655,12,0.2466156867461008,0.345,0.4114911906791702,0.1902675075642405,4.99329483660571,5.041623525928673
tanh,32,1000,"[16,32,16]",0.01,mini-batch,0.4975,11,0.3882232176454856,0.5025,0.320990905085276,0.4985243922484104,5.788075795536461,5.681842188719091
tanh,32,1000,"[16,32,16]",0.01,batch,0.6837500000000001,1000,0.3292682926829268,0.31625,0.6736111111111112,0.0514112903225806,5.061945989418579,4.971529394616471
tanh,32,1000,"[16,32,16]",0.01,sgd,0.67625,26,0.3835169973930951,0.32375,0.4047619047619047,0.0955115092278186,5.024568276484556,4.967662797972688
tanh,32,1000,"[64,32]",0.1,mini-batch,0.5237499999999999,11,0.3884069767385712,0.47625,0.3341789217888725,0.4736946599050863,5.713691851698535,5.645412526585732
tanh,32,1000,"[64,32]",0.1,batch,0.6837500000000001,76,0.2305274382480265,0.31625,0.6961032388663968,0.0779084845950264,5.043916536201342,4.942256215049856
tanh,32,1000,"[64,32]",0.1,sgd,0.64375,15,0.2818757706658882,0.35625,0.4156542076139263,0.226621670763193,4.904944962498956,5.096925081134924
tanh,32,1000,"[64,32]",0.01,mini-batch,0.5237499999999999,11,0.3884069767385712,0.47625,0.3341789217888725,0.4736946599050863,5.713691851698535,5.645412526585732
tanh,32,1000,"[64,32]",0.01,batch,0.685,674,0.1055274382480264,0.315,0.8211032388663968,0.0779084845950264,5.046886498205977,4.941871518643393
tanh,32,1000,"[64,32]",0.01,sgd,0.6799999999999999,16,0.1449030168449601,0.32,0.593859649122807,0.1062576954357483,5.0348010328509485,4.9555522580031575
tanh,32,1000,[16],0.1,mini-batch,0.4975,11,0.3748163729286347,0.5025,0.3139447152170533,0.4703803111510591,5.946434882125978,5.963450127849592
tanh,32,1000,[16],0.1,batch,0.6799999999999999,221,0.130998334682207,0.32,0.6977591036414565,0.0914604185854685,5.036800298386169,4.95855914564903
tanh,32,1000,[16],0.1,sgd,0.66875,12,0.2043827226621026,0.33125,0.4458333333333333,0.1465118436310574,5.026914633930776,5.024334924424032
tanh,32,1000,[16],0.01,mini-batch,0.4975,11,0.3748163729286347,0.5025,0.3139447152170533,0.4703803111510591,5.946434882125978,5.963450127849592
tanh,32,1000,[16],0.01,batch,0.67625,1000,0.1071149379076208,0.32375,0.7335858585858586,0.0676865358598636,5.069924898441096,4.981831369543553
tanh,32,1000,[16],0.01,sgd,0.67875,31,0.388969178538144,0.32125,0.5426587301587301,0.0992723464126265,5.027708425481517,4.961994340083397
tanh,32,100,"[16,32,16]",0.1,mini-batch,0.4975,11,0.3882232176454856,0.5025,0.320990905085276,0.4985243922484104,5.788075795536461,5.681842188719091
tanh,32,100,"[16,32,16]",0.1,batch,0.6837500000000001,100,0.3292682926829268,0.31625,0.6736111111111112,0.0514112903225806,5.0620166212627655,4.97128372363239
tanh,32,100,"[16,32,16]",0.1,sgd,0.655,12,0.2466156867461008,0.345,0.4114911906791702,0.1902675075642405,4.99329483660571,5.041623525928673
tanh,32,100,"[16,32,16]",0.01,mini-batch,0.4975,11,0.3882232176454856,0.5025,0.320990905085276,0.4985243922484104,5.788075795536461,5.681842188719091
tanh,32,100,"[16,32,16]",0.01,batch,0.6425000000000001,100,0.2111041457729565,0.3575,0.3537425889328063,0.1694257419058927,5.3259440324112814,5.201966653451157
tanh,32,100,"[16,32,16]",0.01,sgd,0.67625,26,0.3835169973930951,0.32375,0.4047619047619047,0.0955115092278186,5.024568276484556,4.967662797972688
tanh,32,100,"[64,32]",0.1,mini-batch,0.5237499999999999,11,0.3884069767385712,0.47625,0.3341789217888725,0.4736946599050863,5.713691851698535,5.645412526585732
tanh,32,100,"[64,32]",0.1,batch,0.6837500000000001,76,0.2305274382480265,0.31625,0.6961032388663968,0.0779084845950264,5.043916536201342,4.942256215049856
tanh,32,100,"[64,32]",0.1,sgd,0.64375,15,0.2818757706658882,0.35625,0.4156542076139263,0.226621670763193,4.904944962498956,5.096925081134924
tanh,32,100,"[64,32]",0.01,mini-batch,0.5237499999999999,11,0.3884069767385712,0.47625,0.3341789217888725,0.4736946599050863,5.713691851698535,5.645412526585732
tanh,32,100,"[64,32]",0.01,batch,0.67875,100,0.2103985962179239,0.32125,0.5021008403361344,0.1466454662179906,5.21764164620472,5.115372143238818
tanh,32,100,"[64,32]",0.01,sgd,0.6799999999999999,16,0.1449030168449601,0.32,0.593859649122807,0.1062576954357483,5.0348010328509485,4.9555522580031575
tanh,32,100,[16],0.1,mini-batch,0.4975,11,0.3748163729286347,0.5025,0.3139447152170533,0.4703803111510591,5.946434882125978,5.963450127849592
tanh,32,100,[16],0.1,batch,0.67625,100,0.1071149379076208,0.32375,0.7335858585858586,0.0676865358598636,5.069896857066088,4.98158258265437
tanh,32,100,[16],0.1,sgd,0.66875,12,0.2043827226621026,0.33125,0.4458333333333333,0.1465118436310574,5.026914633930776,5.024334924424032
tanh,32,100,[16],0.01,mini-batch,0.4975,11,0.3748163729286347,0.5025,0.3139447152170533,0.4703803111510591,5.946434882125978,5.963450127849592
tanh,32,100,[16],0.01,batch,0.55625,100,0.3350190881699572,0.44375,0.3294914580440896,0.3512490966604525,5.586321893978202,5.578541330346592
tanh,32,100,[16],0.01,sgd,0.67875,31,0.388969178538144,0.32125,0.5426587301587301,0.0992723464126265,5.027708425481517,4.961994340083397
relu,128,1000,"[16,32,16]",0.1,mini-batch,0.495,11,0.42783251441417,0.505,0.3699205301992876,0.5341560600848526,5.780282606904832,5.706672088151206
relu,128,1000,"[16,32,16]",0.1,batch,0.675,277,0.5934188034188035,0.325,0.4026315789473684,0.0625889468690702,5.348873548995478,5.288569977930101
relu,128,1000,"[16,32,16]",0.1,sgd,0.6837500000000001,39,0.1119046268535775,0.31625,0.84375,0.0728946900279636,5.646426687593325,5.474930243664694
relu,128,1000,"[16,32,16]",0.01,mini-batch,0.495,11,0.42783251441417,0.505,0.3699205301992876,0.5341560600848526,5.780282606904832,5.706672088151206
relu,128,1000,"[16,32,16]",0.01,batch,0.665,1000,0.5452380952380953,0.335,0.4204545454545454,0.0278477822580645,5.359413965730371,5.296385056935971
relu,128,1000,"[16,32,16]",0.01,sgd,0.67125,34,0.4786324786324786,0.32875,0.4244047619047619,0.070787871600253,5.350338992985337,5.290953279771504
relu,128,1000,"[64,32]",0.1,mini-batch,0.495,11,0.2992614924611855,0.505,0.2878658972602271,0.4409508119768375,5.671314229158263,5.68141366475
relu,128,1000,"[64,32]",0.1,batch,0.6625,172,0.426191821234661,0.3375,0.4,0.031137255942275,5.285008417214049,5.175007789872277
relu,128,1000,"[64,32]",0.1,sgd,0.6699999999999999,24,0.3262226512226512,0.33,0.5381944444444444,0.0544487478777589,5.3110666897276015,5.211070577847142
relu,128,1000,"[64,32]",0.01,mini-batch,0.495,11,0.2992614924611855,0.505,0.2878658972602271,0.4409508119768375,5.671314229158263,5.68141366475
relu,128,1000,"[64,32]",0.01,batch,0.6675,1000,0.5525778226206624,0.3325,0.2958333333333333,0.031137255942275,5.289920663189448,5.178114899097661
relu,128,1000,"[64,32]",0.01,sgd,0.66125,157,0.6065223694187966,0.33875,0.2890625,0.0831821236559139,5.239663784825578,5.062464613841279
relu,128,1000,[16],0.1,mini-batch,0.4825,11,0.3763644411703959,0.5175,0.3172502107272072,0.5135784591142626,5.9446904800095375,5.974920112269478
relu,128,1000,[16],0.1,batch,0.6737500000000001,246,0.2388561522169843,0.32625,0.4733630952380953,0.0708340008683156,5.274405688292585,5.174398596728269
relu,128,1000,[16],0.1,sgd,0.69875,18,0.1301333691371285,0.30125,0.8186274509803921,0.0879768482685471,5.411807767542707,5.174521977172751
relu,128,1000,[16],0.01,mini-batch,0.4825,11,0.3763644411703959,0.5175,0.3172502107272072,0.5135784591142626,5.9446904800095375,5.974920112269478
relu,128,1000,[16],0.01,batch,0.6675,1000,0.2288264672714496,0.3325,0.4169642857142857,0.0632630849571483,5.288203561118685,5.185376999288035
relu,128,1000,[16],0.01,sgd,0.685,101,0.1974991905224463,0.315,0.7154761904761905,0.1473323105888271,5.2514555657900654,5.117706002767334
relu,128,100,"[16,32,16]",0.1,mini-batch,0.495,11,0.42783251441417,0.505,0.3699205301992876,0.5341560600848526,5.780282606904832,5.706672088151206
relu,128,100,"[16,32,16]",0.1,batch,0.66625,100,0.4271825396825396,0.33375,0.4829545454545454,0.0315242528462998,5.358477949995498,5.2954083434767885
relu,128,100,"[16,32,16]",0.1,sgd,0.6837500000000001,39,0.1119046268535775,0.31625,0.84375,0.0728946900279636,5.646426687593325,5.474930243664694
relu,128,100,"[16,32,16]",0.01,mini-batch,0.495,11,0.42783251441417,0.505,0.3699205301992876,0.5341560600848526,5.780282606904832,5.706672088151206
relu,128,100,"[16,32,16]",0.01,batch,0.63,100,0.2620965541483673,0.37,0.3951711024448093,0.2142445174282691,5.446605977654811,5.3821040551190364
relu,128,100,"[16,32,16]",0.01,sgd,0.67125,34,0.4786324786324786,0.32875,0.4244047619047619,0.070787871600253,5.350338992985337,5.290953279771504
relu,128,100,"[64,32]",0.1,mini-batch,0.495,11,0.2992614924611855,0.505,0.2878658972602271,0.4409508119768375,5.671314229158263,5.68141366475
relu,128,100,"[64,32]",0.1,batch,0.6675,100,0.5525778226206624,0.3325,0.2958333333333333,0.031137255942275,5.289168158862262,5.177219638564368
relu,128,100,"[64,32]",0.1,sgd,0.6699999999999999,24,0.3262226512226512,0.33,0.5381944444444444,0.0544487478777589,5.3110666897276015,5.211070577847142
relu,128,100,"[64,32]",0.01,mini-batch,0.495,11,0.2992614924611855,0.505,0.2878658972602271,0.4409508119768375,5.671314229158263,5.68141366475
relu,128,100,"[64,32]",0.01,batch,0.625,100,0.3978833666333666,0.375,0.2249188311688311,0.1260335262075967,5.358507160002492,5.275536823849068
relu,128,100,"[64,32]",0.01,sgd,0.67125,100,0.6102663486384416,0.32875,0.30625,0.0831821236559139,5.22639245916489,5.093338147878392
relu,128,100,[16],0.1,mini-batch,0.4825,11,0.3763644411703959,0.5175,0.3172502107272072,0.5135784591142626,5.9446904800095375,5.974920112269478
relu,128,100,[16],0.1,batch,0.6675,100,0.2243798269552441,0.3325,0.4148809523809524,0.059586614368913,5.287559212757416,5.183814059024362
relu,128,100,[16],0.1,sgd,0.69875,18,0.1301333691371285,0.30125,0.8186274509803921,0.0879768482685471,5.411807767542707,5.174521977172751
relu,128,100,[16],0.01,mini-batch,0.4825,11,0.3763644411703959,0.5175,0.3172502107272072,0.5135784591142626,5.9446904800095375,5.974920112269478
relu,128,100,[16],0.01,batch,0.58375,100,0.2757455583209893,0.41625,0.3173327664399092,0.2614321029652193,5.479856913684637,5.435027211212777
relu,128,100,[16],0.01,sgd,0.6837500000000001,100,0.1960478117125769,0.31625,0.7107142857142856,0.1473323105888271,5.250262853967067,5.117004532484295
relu,32,1000,"[16,32,16]",0.1,mini-batch,0.495,11,0.42783251441417,0.505,0.3699205301992876,0.5341560600848526,5.780282606904832,5.706672088151206
relu,32,1000,"[16,32,16]",0.1,batch,0.675,277,0.5934188034188035,0.325,0.4026315789473684,0.0625889468690702,5.348873548995478,5.288569977930101
relu,32,1000,"[16,32,16]",0.1,sgd,0.6837500000000001,39,0.1119046268535775,0.31625,0.84375,0.0728946900279636,5.646426687593325,5.474930243664694
relu,32,1000,"[16,32,16]",0.01,mini-batch,0.495,11,0.42783251441417,0.505,0.3699205301992876,0.5341560600848526,5.780282606904832,5.706672088151206
relu,32,1000,"[16,32,16]",0.01,batch,0.665,1000,0.5452380952380953,0.335,0.4204545454545454,0.0278477822580645,5.359413965730371,5.296385056935971
relu,32,1000,"[16,32,16]",0.01,sgd,0.67125,34,0.4786324786324786,0.32875,0.4244047619047619,0.070787871600253,5.350338992985337,5.290953279771504
relu,32,1000,"[64,32]",0.1,mini-batch,0.495,11,0.2992614924611855,0.505,0.2878658972602271,0.4409508119768375,5.671314229158263,5.68141366475
relu,32,1000,"[64,32]",0.1,batch,0.6625,172,0.4261918212346609,0.3375,0.4,0.031137255942275,5.285008417214049,5.175007789872277
relu,32,1000,"[64,32]",0.1,sgd,0.6699999999999999,24,0.3262226512226512,0.33,0.5381944444444444,0.0544487478777589,5.3110666897276015,5.211070577847142
relu,32,1000,"[64,32]",0.01,mini-batch,0.495,11,0.2992614924611855,0.505,0.2878658972602271,0.4409508119768375,5.671314229158263,5.68141366475
relu,32,1000,"[64,32]",0.01,batch,0.6675,1000,0.5525778226206624,0.3325,0.2958333333333333,0.031137255942275,5.289920663189448,5.178114899097661
relu,32,1000,"[64,32]",0.01,sgd,0.66125,157,0.6065223694187966,0.33875,0.2890625,0.0831821236559139,5.239663784825578,5.062464613841279
relu,32,1000,[16],0.1,mini-batch,0.4825,11,0.3763644411703959,0.5175,0.3172502107272072,0.5135784591142626,5.9446904800095375,5.974920112269478
relu,32,1000,[16],0.1,batch,0.6737500000000001,246,0.2388561522169843,0.32625,0.4733630952380953,0.0708340008683156,5.274405688292585,5.174398596728269
relu,32,1000,[16],0.1,sgd,0.69875,18,0.1301333691371285,0.30125,0.8186274509803921,0.0879768482685471,5.411807767542707,5.174521977172751
relu,32,1000,[16],0.01,mini-batch,0.4825,11,0.3763644411703959,0.5175,0.3172502107272072,0.5135784591142626,5.9446904800095375,5.974920112269478
relu,32,1000,[16],0.01,sgd,0.685,101,0.1974991905224463,0.315,0.7154761904761905,0.1473323105888271,5.2514555657900654,5.117706002767334
relu,32,1000,[16],0.01,batch,0.6675,1000,0.2288264672714496,0.3325,0.4169642857142857,0.0632630849571483,5.288203561118685,5.185376999288035
relu,32,1000,[16],0.01,sgd,0.685,101,0.1974991905224463,0.315,0.7154761904761905,0.1473323105888271,5.2514555657900654,5.117706002767334
relu,32,100,"[16,32,16]",0.1,mini-batch,0.495,11,0.42783251441417,0.505,0.3699205301992876,0.5341560600848526,5.780282606904832,5.706672088151206
relu,32,100,"[16,32,16]",0.1,batch,0.66625,100,0.4271825396825396,0.33375,0.4829545454545454,0.0315242528462998,5.358477949995498,5.2954083434767885
relu,32,100,"[16,32,16]",0.1,sgd,0.6837500000000001,39,0.1119046268535775,0.31625,0.84375,0.0728946900279636,5.646426687593325,5.474930243664694
relu,32,100,"[16,32,16]",0.01,mini-batch,0.495,11,0.42783251441417,0.505,0.3699205301992876,0.5341560600848526,5.780282606904832,5.706672088151206
relu,32,100,"[16,32,16]",0.01,batch,0.63,100,0.2620965541483673,0.37,0.3951711024448093,0.2142445174282691,5.446605977654811,5.3821040551190364
relu,32,100,"[16,32,16]",0.01,sgd,0.67125,34,0.4786324786324786,0.32875,0.4244047619047619,0.070787871600253,5.350338992985337,5.290953279771504
relu,32,100,"[64,32]",0.1,mini-batch,0.495,11,0.2992614924611855,0.505,0.2878658972602271,0.4409508119768375,5.671314229158263,5.68141366475
relu,32,100,"[64,32]",0.1,batch,0.6675,100,0.5525778226206624,0.3325,0.2958333333333333,0.031137255942275,5.289168158862262,5.177219638564368
relu,32,100,"[64,32]",0.1,sgd,0.6699999999999999,24,0.3262226512226512,0.33,0.5381944444444444,0.0544487478777589,5.3110666897276015,5.211070577847142
relu,32,100,"[64,32]",0.01,mini-batch,0.495,11,0.2992614924611855,0.505,0.2878658972602271,0.4409508119768375,5.671314229158263,5.68141366475
relu,32,100,"[64,32]",0.01,batch,0.625,100,0.3978833666333666,0.375,0.2249188311688311,0.1260335262075967,5.358507160002492,5.275536823849068
relu,32,100,"[64,32]",0.01,sgd,0.67125,100,0.6102663486384416,0.32875,0.30625,0.0831821236559139,5.22639245916489,5.093338147878392
relu,32,100,[16],0.1,mini-batch,0.4825,11,0.3763644411703959,0.5175,0.3172502107272072,0.5135784591142626,5.9446904800095375,5.974920112269478
relu,32,100,[16],0.1,batch,0.6675,100,0.2243798269552441,0.3325,0.4148809523809524,0.059586614368913,5.287559212757416,5.183814059024362
relu,32,100,[16],0.1,sgd,0.69875,18,0.1301333691371285,0.30125,0.8186274509803921,0.0879768482685471,5.411807767542707,5.174521977172751
relu,32,100,[16],0.01,mini-batch,0.4825,11,0.3763644411703959,0.5175,0.3172502107272072,0.5135784591142626,5.9446904800095375,5.974920112269478
relu,32,100,[16],0.01,batch,0.58375,100,0.2757455583209893,0.41625,0.3173327664399092,0.2614321029652193,5.479856913684637,5.435027211212777
relu,32,100,[16],0.01,sgd,0.6837500000000001,100,0.1960478117125769,0.31625,0.7107142857142856,0.1473323105888271,5.250262853967067,5.117004532484295
```

**Best Model Parameters**

```python
{
  "input_size": 9,
  "output_size": 8,
  "hidden_layers": [64, 32],
  "activation": "sigmoid",
  "optimizer": "sgd",
  "learning_rate": 0.01,
  "batch_size": 32,
  "epochs": 1000,
  "early_stopping": true,
  "patience": 10,
  "wandb_log": true
}
```

**Evaluation on Test**

```python
Test Metrics: {
	'accuracy': 0.67625,
	'precision': 0.917016806722689,
	'recall': 0.07291666666666666,
	'f1_score': 0.1015139751552795,
	'hamming_loss': 0.32375
	}
```

### 2.7 Analysis

```python
Class-wise Performance Metrics:
beauty: Accuracy: 0.6600, Precision: 0.0000, Recall: 0.0000, F1 Score: 0.0000
books: Accuracy: 0.7200, Precision: 0.0000, Recall: 0.0000, F1 Score: 0.0000
clothing: Accuracy: 0.7000, Precision: 0.5714, Recall: 0.2500, F1 Score: 0.3478
electronics: Accuracy: 0.6400, Precision: 0.0000, Recall: 0.0000, F1 Score: 0.0000
food: Accuracy: 0.7100, Precision: 0.0000, Recall: 0.0000, F1 Score: 0.0000
furniture: Accuracy: 0.6100, Precision: 0.0000, Recall: 0.0000, F1 Score: 0.0000
home: Accuracy: 0.6700, Precision: 0.0000, Recall: 0.0000, F1 Score: 0.0000
sports: Accuracy: 0.7000, Precision: 0.7647, Recall: 0.3333, F1 Score: 0.4643
```

### Classes where the model performed well:

1. **Clothing**:
   - **Precision**: 0.5714
   - **Recall**: 0.2500
   - **F1 Score**: 0.3478
   - While recall is low, the precision is relatively high. This indicates that when the model predicted "clothing," it was often correct, but it struggled to identify all "clothing" instances (low recall).
2. **Sports**:
   - **Precision**: 0.7647
   - **Recall**: 0.3333
   - **F1 Score**: 0.4643
   - Precision is the highest across all classes, which indicates that the model was highly confident when predicting "sports," although it still missed many true positives, resulting in a lower recall.

### Classes where the model struggled:

1. **Beauty**, **Books**, **Electronics**, **Food**, **Furniture**, **Home**:
   - For all these classes, the model's precision, recall, and F1 score are 0. This means that the model was unable to make correct predictions for these classes, suggesting that it had difficulty distinguishing these categories.
   - For example, in the case of "beauty" and "electronics," the model did not identify any true positives at all, leading to F1 scores of 0. These classes had neither precision nor recall, indicating the model’s complete failure to recognize them.

**Key Factors Influencing Class Performance**

- **Disparities in Class Distribution:** Differences in sample sizes among classes can lead the model to prioritize those with more examples, resulting in inadequate attention to underrepresented classes during training.
- **Inadequate Feature Differentiation:** The features employed in training may not effectively capture the distinctions between classes that are struggling, contributing to misclassifications.
- **Model Capacity Issues:** The model might lack the necessary complexity to comprehend the nuances of certain classes or may be overly complex, leading to overfitting on the more frequent classes.
- **Inconsistent Labeling:** In multi-label classification scenarios, the presence of noisy or inconsistent labels can mislead the model, particularly impacting classes like **Beauty**.
- **Insufficient Sample Sizes:** Classes with limited training instances may not generalize effectively, resulting in a higher incidence of false negatives and lower precision and recall scores.

## **Multilayer Perceptron Regression**

### 3.1 Data Preprocessing

![image.png](Statistical%20Methods%20in%20Artificial%20Intelligence%2011b17aaec13280909daef4fe08a2d27a/image%207.png)

![image.png](Statistical%20Methods%20in%20Artificial%20Intelligence%2011b17aaec13280909daef4fe08a2d27a/image%208.png)

| **Column Name** | **Mean** | **Min** | **Max** | **Std** |
| --------------- | -------- | ------- | ------- | ------- |
| CRIM            | 3.61     | 0.01    | 88.98   | 8.72    |
| ZN              | 11.21    | 0.00    | 100.00  | 23.39   |
| INDUS           | 11.08    | 0.46    | 27.74   | 6.84    |
| CHAS            | 0.07     | 0.00    | 1.00    | 0.26    |
| NOX             | 0.55     | 0.39    | 0.87    | 0.12    |
| RM              | 6.28     | 3.56    | 8.78    | 0.70    |
| AGE             | 68.52    | 2.90    | 100.00  | 28.00   |
| DIS             | 3.80     | 1.13    | 12.13   | 2.11    |
| RAD             | 9.55     | 1.00    | 24.00   | 8.71    |
| TAX             | 408.24   | 187.00  | 711.00  | 168.54  |
| PTRATIO         | 18.46    | 12.60   | 22.00   | 2.16    |
| B               | 356.67   | 0.32    | 396.90  | 91.29   |
| LSTAT           | 12.72    | 1.73    | 37.97   | 7.16    |
| MEDV            | 22.53    | 5.00    | 50.00   | 9.20    |

### 3.2 MLP Regression Implementation from Scratch

(Done bonus: Classifier and Regression handled by single class)

Created a common class to accommodate both MLP Classification and MLP Regression tasks, implementing specific modifications for regression. These modifications include:

- **Architecture**: The output layer has been adjusted to consist of a single neuron to provide one output.
- **Activation Function**: The output layer utilizes a linear activation function, or it may operate without an activation function.
- **Loss Function**: The Mean Squared Error (MSE) loss function is employed for regression tasks.

### 3.3 Model Training & Hyperparameter Tuning using W&B

https://wandb.ai/vcnk4v/MLP-regression-first-r?nw=nwuservcnk4v : First Run

https://wandb.ai/vcnk4v/MLP-HyperparameterTuningRegression-fi/sweeps/3i86qmu6?nw=nwuservcnk4v : Hyperparameter Tuning

```python
activation,layers,learning_rate,optimizer,MAE,MSE,R2,RMSE,epoch,train_loss,val_loss
relu,"[64,32,16,8]",0.001,mini_batch,0.7550161013549777,1.074071517247713,0.0896814386682349,1.0363742167999517,14,0.8908470260088602,1.074071517247713
relu,"[64,32,16,8]",0.001,batch,0.7446374604123263,1.057288716941288,-0.0244197213022108,1.0282454555899034,58,1.0148180701391696,1.057288716941288
relu,"[64,32,16,8]",0.001,sgd,0.8428084530265028,1.4104663264789532,0.5137964815305207,1.1876305513411791,11,0.4542262484653953,1.4104663264789532
relu,"[64,32,16,8]",0.01,mini_batch,0.8319749832724137,1.3202928167698134,0.4552690939243847,1.1490399543835772,11,0.5319078887875134,1.3202928167698134
relu,"[64,32,16,8]",0.01,batch,0.7514010935034526,1.068243144313478,0.0665673316648357,1.0335584861600615,15,0.9206312094175868,1.068243144313478
relu,"[64,32,16,8]",0.01,sgd,0.8321559320243832,1.4978393306980131,0.7186072060123169,1.2238624639631748,14,0.3359214624669953,1.4978393306980131
relu,"[32,16]",0.001,mini_batch,0.793770841166588,1.1883085026611877,0.282352738589369,1.090095639226755,17,0.7651762684946584,1.1883085026611877
relu,"[32,16]",0.001,batch,0.7827272149763808,1.1584555460331325,0.1234256433095272,1.0763157278573665,100,0.880725496902755,1.1584555460331325
relu,"[32,16]",0.001,sgd,1.0392835754185767,1.9275171524459969,0.6463083470553526,1.3883505149802755,11,0.5087329558021192,1.9275171524459969
relu,"[32,16]",0.01,mini_batch,0.9230431072381649,1.5348678934130244,0.6603333345514639,1.2388978543096378,11,0.4769340758107653,1.5348678934130244
relu,"[32,16]",0.01,batch,0.7894528185410178,1.179315319999471,0.2555128088102413,1.085962853876444,19,0.7868879380706355,1.179315319999471
relu,"[16,32,16]",0.001,mini_batch,0.7231467769646652,1.0495226865003675,0.1095153957825357,1.0244621449816325,33,0.9188810107002644,1.0495226865003675
relu,"[16,32,16]",0.001,batch,0.7314345333098641,1.0645065553106712,-0.0389465878605406,1.0317492695954145,100,1.0676435441263863,1.0645065553106712
relu,"[16,32,16]",0.001,sgd,0.8405766793304777,1.4090800818161584,0.70721821092952,1.1870467900702812,11,0.3338562743445606,1.4090800818161584
relu,"[16,32,16]",0.01,mini_batch,0.8100722194274613,1.215024595544567,0.4075108720083472,1.102281540961549,12,0.590374855247506,1.215024595544567
relu,"[16,32,16]",0.01,batch,0.719497075862129,1.0459408860662254,0.1037778715685613,1.022712513889522,40,0.9301540142659002,1.0459408860662254
relu,"[16,32,16]",0.01,sgd,0.9900325964169508,1.871395585396047,0.7571702365535276,1.3679896145059167,11,0.2316477551737972,1.871395585396047
relu,"[64,32]",0.001,mini_batch,0.8618729364741797,1.4780969912621775,-0.4712492877453549,1.2157701227050193,11,1.661265678441144,1.4780969912621775
relu,"[64,32]",0.001,batch,0.8585884730902716,1.468701528139741,-0.4583816991978191,1.211899966226479,11,1.6472880296950474,1.468701528139741
relu,"[64,32]",0.001,sgd,0.9068297350065506,1.6010974000978584,-0.6107633871228759,1.2653447751889042,11,1.81057361452822,1.6010974000978584
relu,"[64,32]",0.01,mini_batch,0.8913363053990664,1.5596244289570231,-0.5734600529813896,1.2488492418851136,11,1.7712519511665337,1.5596244289570231
relu,"[64,32]",0.01,batch,0.8610990262745091,1.4758869781558066,-0.4681369861971196,1.2148608883966123,11,1.657939487455729,1.4758869781558066
relu,"[64,32]",0.01,sgd,0.9672692476269764,1.75996225672838,-0.8052157875036794,1.3266356910351764,11,2.005363190426664,1.75996225672838
relu,[16],0.001,mini_batch,0.8628202100965415,1.374089649072093,0.5524058863911386,1.1722157007445742,38,0.6392180999013309,1.374089649072093
relu,[16],0.001,batch,0.9704382867734112,1.5725793601824822,0.0522830297881736,1.254025262976182,100,1.1106766823346823,1.5725793601824822
relu,[16],0.001,sgd,0.942840451890565,1.7779464008878088,0.7380320361823459,1.3333965655002298,11,0.3493414997587237,1.7779464008878088
relu,[16],0.01,mini_batch,0.9027229947948304,1.6023773381393371,0.7266110405001994,1.2658504406679871,13,0.4307293902173292,1.6023773381393371
relu,[16],0.01,batch,0.8623100948387618,1.3685507237430852,0.5378548893006891,1.169850727119954,46,0.6541267790813193,1.3685507237430852
relu,[16],0.01,sgd,1.164939885763898,2.436362784473654,0.5647149967172782,1.560885256664837,11,0.2925295122749604,2.436362784473654
tanh,"[64,32,16,8]",0.001,mini_batch,0.8655316337816553,1.400598771045641,0.6822693870502881,1.183468956519621,11,0.5346540810410507,1.400598771045641
tanh,"[64,32,16,8]",0.001,batch,0.7981783572683946,1.1959544496266956,0.2074386906348056,1.093597023417079,22,1.1123476435330055,1.1959544496266956
tanh,"[64,32,16,8]",0.001,sgd,0.9195920114381828,1.523016106464027,0.7659723753036981,1.2341053870978875,11,0.2551714012389444,1.523016106464027
tanh,"[64,32,16,8]",0.01,mini_batch,0.9480169024907056,1.5849772174103338,0.7615125562456571,1.2589587830466626,11,0.2786746395661065,1.5849772174103338
tanh,"[64,32,16,8]",0.01,batch,0.8624326847509793,1.3894577783796838,0.645526533956678,1.178752636637426,11,0.6077615869448977,1.3894577783796838
tanh,"[64,32,16,8]",0.01,sgd,0.8721601992088939,1.424373719512784,0.7511377612744277,1.193471289773149,17,0.2075265477317093,1.424373719512784
tanh,"[32,16]",0.001,mini_batch,0.9031860539751534,1.410238934571118,0.2630661241158802,1.187534814045937,11,0.8147212960105322,1.410238934571118
tanh,"[32,16]",0.001,batch,0.8528623880660446,1.3058095376566916,-0.1778596944665851,1.1427202359530926,20,1.1921926955222997,1.3058095376566916
tanh,"[32,16]",0.001,sgd,0.9493201537764192,1.601557210558468,0.7568297563922021,1.265526455890381,11,0.2677582185978158,1.601557210558468
tanh,"[32,16]",0.01,mini_batch,0.9488732040764072,1.6070327534192528,0.719870095131367,1.267687955854773,11,0.3406578271952151,1.6070327534192528
tanh,"[32,16]",0.01,batch,0.8959183493249039,1.389353117212052,0.1972137690484728,1.1787082409197165,11,0.8722674954100365,1.389353117212052
tanh,"[32,16]",0.01,sgd,0.9062735523422292,1.5195235913493217,0.7908419378068339,1.232689576231308,11,0.2066072104418561,1.5195235913493217
tanh,"[16,32,16]",0.001,mini_batch,0.8152648864586739,1.2219055457385493,0.4908011728215711,1.1053983651781605,24,0.6099778815747222,1.2219055457385493
tanh,"[16,32,16]",0.001,batch,0.811309975292633,1.2188840954504916,-0.1438461935948769,1.104030839900087,100,1.1178487230482648,1.2188840954504916
tanh,"[16,32,16]",0.001,sgd,0.9638892879277856,1.649449061442503,0.7799478662104307,1.2843087874193273,11,0.2940066163397158,1.649449061442503
tanh,"[16,32,16]",0.01,mini_batch,0.9266567214915432,1.5492111172097855,0.7449374560434031,1.2446730965236557,11,0.3484507630540093,1.5492111172097855
tanh,"[16,32,16]",0.01,batch,0.8122348370836939,1.208405049714195,0.446455375857411,1.099274783534215,28,0.647508928887676,1.208405049714195
tanh,"[16,32,16]",0.01,sgd,0.8894393708277281,1.5034326647915337,0.851332730561668,1.2261454500961677,16,0.1888750133825117,1.5034326647915337
tanh,"[64,32]",0.001,mini_batch,1.0384021079494716,1.8058284318763973,0.4245925637151526,1.343811159306395,11,0.6018059586656399,1.8058284318763973
tanh,"[64,32]",0.001,batch,0.965721323907264,1.5982811971552635,-0.0021668940246644,1.2642314650234203,11,1.0528436442164522,1.5982811971552635
tanh,"[64,32]",0.001,sgd,0.937841074373563,1.5805884582970875,0.7564010591728907,1.257214563349108,21,0.2583981309747236,1.5805884582970875
tanh,"[64,32]",0.01,mini_batch,0.9646337855051024,1.6479845468198009,0.7548505340582028,1.2837385040652949,39,0.272880311550458,1.6479845468198009
tanh,"[64,32]",0.01,batch,1.0428524248139257,1.809676360440717,0.37604318031808,1.345242119635241,11,0.6531795162928312,1.809676360440717
tanh,"[64,32]",0.01,sgd,0.9021383532737668,1.5070180555059876,0.7899397803411142,1.227606637122001,16,0.2088805280170902,1.5070180555059876
tanh,[16],0.001,mini_batch,1.05305821052888,1.8630505557882329,0.0815765577900546,1.3649360995256272,11,1.0454630638960027,1.8630505557882329
tanh,[16],0.001,batch,1.0223593162920284,1.7681444584742478,-0.1180103488919845,1.32971593149599,11,1.2240454054845529,1.7681444584742478
tanh,[16],0.001,sgd,0.9775523741562396,1.7179273139522164,0.8230480428127591,1.3106972625103848,59,0.2347845900429112,1.7179273139522164
tanh,[16],0.01,mini_batch,1.0999053653307234,2.073818515051248,0.4866340667875415,1.4400758712829154,11,0.5363984923064654,2.073818515051248
tanh,[16],0.01,batch,1.0481002888841748,1.847238714590286,0.0444065255662409,1.3591316031165952,11,1.0824875584184177,1.847238714590286
tanh,[16],0.01,sgd,0.919166876659451,1.5660713426832735,0.8168400583201312,1.2514277217175882,15,0.2282909295906214,1.5660713426832735
sigmoid,"[64,32,16,8]",0.001,mini_batch,0.7401434905387138,1.0517566310210544,-0.0689219326613892,1.0255518665679737,100,1.0342877366544756,1.0517566310210544
sigmoid,"[64,32,16,8]",0.001,batch,0.8382178789965734,1.1642677120102565,-0.1835937139921539,1.0790123780616496,100,1.0840450539970834,1.1642677120102565
sigmoid,"[64,32,16,8]",0.001,sgd,0.7111105142359632,1.0196145818833242,-0.029899415615658,1.0097596654072316,100,1.0244238583494478,1.0196145818833242
sigmoid,"[64,32,16,8]",0.01,mini_batch,0.7115543871497599,1.020057791607931,-0.0356479511322465,1.0099791045402529,60,1.028543431524494,1.020057791607931
sigmoid,"[64,32,16,8]",0.01,batch,0.7525024940358402,1.065293452550122,-0.0827431929459714,1.032130540459937,100,1.0383873135093673,1.065293452550122
sigmoid,"[64,32,16,8]",0.01,sgd,0.703906064761611,1.02052681521909,0.1846339973693153,1.010211272565838,57,0.8650756687475758,1.02052681521909
sigmoid,"[32,16]",0.001,mini_batch,0.6672550096296351,0.9862192677772594,-0.0093259420962064,0.9930857303260678,36,1.0636450664796444,0.9862192677772594
sigmoid,"[32,16]",0.001,batch,0.6542138069709645,0.9926905847901064,-0.0220284091706164,0.9963385894313772,100,1.1250110066383776,0.9926905847901064
sigmoid,"[32,16]",0.001,sgd,0.7071324757936638,1.0179988405377447,0.0531633676739876,1.008959285867247,11,0.9609222641215917,1.0179988405377447
sigmoid,"[32,16]",0.01,mini_batch,0.7030715490749051,1.0126092550031662,-0.0089903749719244,1.0062848776579951,13,1.0158307415459933,1.0126092550031662
sigmoid,"[32,16]",0.01,batch,0.6655577524342562,0.9856488589275756,-0.0095035992074592,0.992798498652962,43,1.067286163635898,0.9856488589275756
sigmoid,"[32,16]",0.01,sgd,0.8622570437050053,1.346646354226159,0.6534268148852046,1.1604509271081476,11,0.4454031664418478,1.346646354226159
sigmoid,"[16,32,16]",0.001,mini_batch,0.7186541868170124,1.0278507220831463,-0.0563940706429129,1.0138297303211947,100,1.0365121349333803,1.0278507220831463
sigmoid,"[16,32,16]",0.001,batch,0.816559728835856,1.137737304201338,-0.1696851956447891,1.066647694508987,100,1.0769092914714058,1.137737304201338
sigmoid,"[16,32,16]",0.001,sgd,0.7092787840637598,1.017940129799967,-0.008643495848726,1.0089301907465982,100,1.0058839068915264,1.017940129799967
sigmoid,"[16,32,16]",0.01,mini_batch,0.711913240104339,1.02061251733277,-0.0449608005568964,1.010253689591268,44,1.0329757462900386,1.02061251733277
sigmoid,"[16,32,16]",0.01,batch,0.7244263783794859,1.034345236750653,-0.0633145828931567,1.0170276479775036,100,1.0374962726502956,1.034345236750653
sigmoid,"[16,32,16]",0.01,sgd,0.706406712591431,1.02994858861391,0.2671998398437185,1.014863827621179,27,0.7860271474099307,1.02994858861391
sigmoid,"[64,32]",0.001,mini_batch,0.6764515115736719,0.9927841324354748,0.0558804102124,0.9963855340356336,50,0.9936380968468712,0.9927841324354748
sigmoid,"[64,32]",0.001,batch,0.8260471380529282,1.3573576161357497,-0.3365049864666198,1.1650569154061745,100,1.6328585487006464,1.3573576161357497
sigmoid,"[64,32]",0.001,sgd,0.7149258869202554,1.0323668647165518,0.1727418086356953,1.0160545579429048,11,0.8662826728961318,1.0323668647165518
sigmoid,"[64,32]",0.01,mini_batch,0.709247664251062,1.021715909123934,0.0753241970030017,1.010799638466464,14,0.94274389412098,1.021715909123934
sigmoid,"[64,32]",0.01,batch,0.6730630104611587,0.9909575285328308,0.0560060047432771,0.9954684970067263,61,0.998660637214682,0.9909575285328308
sigmoid,"[64,32]",0.01,sgd,0.9100838410426612,1.48507065983025,0.726818127063513,1.2186347524300505,11,0.3738480589568988,1.48507065983025
sigmoid,[16],0.001,mini_batch,0.8250536601860062,1.1936460532451287,0.2129572719128064,1.0925410991102935,100,0.8250690886317242,1.1936460532451287
sigmoid,[16],0.001,batch,1.3122397426238586,2.186982408264907,-0.9780386553009466,1.4788449574802989,100,1.8166860800173847,2.186982408264907
sigmoid,[16],0.001,sgd,0.8353778199303009,1.2912688742106782,0.5796057330306916,1.1363401225912415,15,0.5332252866458427,1.2912688742106782
sigmoid,[16],0.01,mini_batch,0.7939429942481491,1.1899464341075174,0.4709276077109832,1.0908466593007091,26,0.6316205610253338,1.1899464341075174
sigmoid,[16],0.01,batch,0.8742817616662141,1.2633913871367757,0.0974640404319694,1.1240068447908915,100,0.9127259133116888,1.2633913871367757
sigmoid,[16],0.01,sgd,0.9644297089136792,1.6649593779667944,0.7388617808869964,1.290333049242247,11,0.3099432339899617,1.6649593779667944

```

**Best Model Parameters:**

```python
{
  "input_size": 13,
  "output_size": 1,
  "hidden_layers": [64, 32],
  "activation": "sigmoid",
  "optimizer": "sgd",
  "learning_rate": 0.01,
  "batch_size": 32,
  "epochs": 100,
  "early_stopping": true,
  "patience": 10,
  "wandb_log": true,
  "task_type": "regression"
}
```

### 3.4 Evaluating Model

```python

# Minimize MSE:
# Mean Squared Error: 0.7643797904074063
# Root Mean Squared Error: 0.8742881621109864
# R-Squared: 0.477
# Mean Absolute Error: 0.6527946163957054

Maximize R^2:
Mean Squared Error: 1.2787771984622123
Root Mean Squared Error: 1.1308303137350946
R-Squared: 0.5728227937365411
Mean Absolute Error: 0.8609607604115237
```

https://wandb.ai/vcnk4v/MLP-regression-test?nw=nwuservcnk4v

## 3.5 Mean Squared Error vs Binary Cross Entropy

In regression class:

![image.png](Statistical%20Methods%20in%20Artificial%20Intelligence%2011b17aaec13280909daef4fe08a2d27a/image%209.png)

![image.png](Statistical%20Methods%20in%20Artificial%20Intelligence%2011b17aaec13280909daef4fe08a2d27a/image%2010.png)

![image.png](Statistical%20Methods%20in%20Artificial%20Intelligence%2011b17aaec13280909daef4fe08a2d27a/image%2011.png)

**1. Mean Squared Error (MSE) Loss:**

- **Loss Curve Analysis**: As shown in Figure 1, the training loss curve for MSE starts at a relatively high value and decreases gradually as the number of epochs increases. This behavior reflects the MLP's ability to minimize the squared differences between predicted and actual values.
- **Convergence**: The MSE loss converges smoothly and consistently throughout the training process, indicating effective learning and adjustment of the model's weights. The curve exhibits stability towards the end, suggesting that the model effectively captures the underlying patterns in the data without significant oscillation.

**2. Binary Cross-Entropy (BCE) Loss:**

- **Loss Curve Analysis**: In contrast, the BCE loss curve shown in Figure 2 exhibits a sharper initial drop followed by fluctuations. This indicates that the model quickly learns to differentiate between the two classes. However, the oscillations in the loss suggest that the model's predictions are sensitive to the distribution of the target values.
- **Convergence**: The BCE loss tends to stabilize at a higher value compared to MSE, which can be attributed to the binary nature of the output and the potential challenges in modeling probabilities accurately. The presence of class imbalance may also contribute to this behavior, leading to variability in the loss as the model adapts.

**3. Comparative Observations:**

- **Initial Drop**: The MSE loss curve demonstrates a more gradual decline, while the BCE curve exhibits a steeper initial drop, reflecting a rapid adjustment in class probability estimations.
- **Final Stability**: MSE results in a smoother and more stable curve towards the end of training, whereas BCE may lead to oscillations, particularly in the presence of imbalanced classes.
- **Sensitivity to Predictions**: MSE captures absolute errors effectively in this regression context, while BCE focuses on the likelihood of class membership, leading to increased variability in loss values.

### Conclusion

This comparison highlights the distinct behaviors of BCE and MSE loss functions in binary classification tasks. While MSE offers a stable convergence profile, BCE may yield quicker initial learning but with increased sensitivity to class distribution.

### 3.6 Analysis

![image.png](Statistical%20Methods%20in%20Artificial%20Intelligence%2011b17aaec13280909daef4fe08a2d27a/image%2012.png)

Bottom 10:

```python
,CRIM,ZN,INDUS,CHAS,NOX,RM,AGE,DIS,RAD,TAX,PTRATIO,B,LSTAT,true_value,prediction,mse_loss
40,-0.3831159297534444,-0.4896385167169952,-0.5534506777306647,-0.2798513233382808,-0.5329422853891227,0.1857265011249988,-1.0367053635458223,0.4402190669563909,-0.5230014462035358,-0.7200617550114154,0.5291583392112035,0.4410519326070325,-0.9414237512998166,0.2685231362336756,0.34509821091747955,0.005863742062830157
35,-0.416457767785177,1.2572091186249732,-0.6983881373148025,3.720376416144203,-0.9303054682859156,0.674384427028926,-1.2993608081871728,0.1343190261724619,-0.6379617988442783,-0.9160580903092445,-0.3955666465973572,0.4410519326070325,-1.311117120794582,1.0739204698714333,1.1472587518853197,0.005378503608748329
46,-0.4082514471370014,0.6021412553717351,-0.8896458159412937,-0.2798513233382808,-0.878475487908073,0.6800830617333443,-0.9163216180852038,1.9898273854450503,-0.1781203882813084,-0.7378796036748544,0.5753945885016305,0.4265789795745643,-0.4589667671715128,0.2685231362336756,0.337122025512492,0.004705807610287313
49,-0.2833363944387372,-0.4896385167169952,1.614634413574325,-0.2798513233382808,0.5986789528604399,0.058931878951676,1.0645381935849791,-0.7244828598885273,-0.6379617988442783,0.1708306781605343,1.268938327858051,0.4410519326070325,-0.0650078097176315,-0.3191992423668502,-0.25080827200181705,0.0046773248274708445
43,-5.20173915561598e-17,-0.4896385167169952,1.0483323807661988,-0.2798513233382808,1.3674903284651043,0.5931788824909551,0.7836427875102017,-0.5692748360875126,1.6612452539705709,1.530926459469711,0.8065758349537706,-1.11226871596098,0.5287854015172042,-0.6674791704264216,-0.6102813922569121,0.0032715858275284104
16,0.1238041690466621,-0.4896385167169952,1.0483323807661988,-0.2798513233382808,1.3674903284651043,-0.4396986576849838,0.7070349494898082,-0.5773085745323432,1.6612452539705709,1.530926459469711,0.8065758349537706,-3.795795424064467,0.8984787710119695,-1.0701778372453004,-1.1026969588191922,0.0010574932679375557
22,-0.4172917529837926,0.9515107824401288,-1.330434997150785,-0.2798513233382808,-0.7143472167115719,0.804028366554458,0.0649883070331739,-0.2910888160925548,-0.2930807409220509,-1.1061151427192604,-0.0256766522739342,0.4410519326070325,-0.7401621099918555,0.6168030642932465,0.6412149468860351,0.0005959400117240947
30,-0.4006577307849999,0.4711276827210874,-2.6542334805936147e-16,0.0,-1.068518749293496,-0.9682470265198432,0.0613403147464887,1.977467787837619,-0.2930807409220509,-0.4646725908354565,0.2979770927590632,0.3558588681659128,0.8199724570265945,-0.4389204676373279,-0.453106862894233,0.00020125381038514027
45,-0.4040803498163971,0.383785300953989,-0.6162071035299821,-0.2798513233382808,-0.7834538572153615,-0.063588767193331,-1.9049275277769528,0.3013637415282859,-0.7529221514850207,-1.1001758598314473,0.0667958463069231,0.4410519326070325,0.0,0.2902906317373987,0.28197818349873954,6.909679572038804e-05
4,1.2558769958084792,-0.4896385167169952,1.0483323807661988,0.0,1.2551920376464454,-2.001124566695783,1.1484420161787436,-1.0484469279448565,1.6612452539705709,1.530926459469711,0.8065758349537706,0.1781266191838597,2.555675689866883,-1.3422715310418405,-1.3421073608277303,2.695185920098663e-08
```

Top 10:

```python
,CRIM,ZN,INDUS,CHAS,NOX,RM,AGE,DIS,RAD,TAX,PTRATIO,B,LSTAT,true_value,prediction,mse_loss
29,0.6583359716205974,-0.4896385167169952,1.0483323807661988,-0.2798513233382808,0.6591472633012564,-0.0977805754198449,1.1484420161787436,-1.2482921138742482,1.6612452539705709,1.530926459469711,0.8065758349537706,0.1038979131006699,-0.4546846045904924,2.989460074199074,-0.45607300147810886,11.871698175585465
32,0.1105728533113222,-0.4896385167169952,1.0483323807661988,-0.2798513233382808,1.4106819787799734,-3.880249360477941,0.7070349494898082,-1.0371806793565437,1.6612452539705709,1.530926459469711,0.8065758349537706,-0.0216439900976341,-0.798684998599135,0.5406168300302154,-1.091789423110296,2.6647501752922436
14,-0.3597584875545763,0.383785300953989,-1.0629730871965513,-0.2798513233382808,0.1754007797747253,1.6887414044155034,-0.5807063277101451,-0.4387860075013611,-0.5230014462035358,-0.8566652614311143,-2.5224341139570474,0.3686871674446916,-1.3639304592938342,2.2820164703280703,1.1704805207137716,1.2355121672849607
28,1.8737991189512937,-7.757561861074651e-17,1.0483323807661988,-0.2798513233382808,1.004680465820207,1.4650699922670585,1.07183417815835,-1.1783177766506363,1.6612452539705709,1.530926459469711,0.8065758349537706,0.4410519326070325,0.1034239184691726,-0.8198516389524837,-0.16907009532262846,0.4235166175292571
27,4.06918316138008,-0.4896385167169952,1.0483323807661988,-0.2798513233382808,1.194723727205629,-1.1847951452877643,1.1484420161787436,-1.0959363049057178,1.6612452539705709,1.530926459469711,0.8065758349537706,0.4410519326070325,2.551393527285862,-1.9082264141386431,-1.262468039267131,0.41700387871669636
44,-0.418912870279978,3.0040567539669416,-1.1122817074674434,-0.2798513233382808,-1.4054136217494724,-0.5821645252954577,-1.8027837437497607,2.578999896008534,-0.9828428567665056,-0.5537618341526515,-0.950401638082495,0.4221932362313924,-0.4946514553466832,-0.1777105215926498,0.4121307719843869,0.34791275160863205
0,-0.4223284613462184,0.2964429191868906,-1.311010389165282,-0.2798513233382808,-0.1442174325553031,0.4136718893017581,-0.1210592995877822,0.140213603492929,-0.9828428567665056,-0.6666082090210984,-1.4590003802772022,0.4410519326070325,-1.1041459293785936,0.1596856587150597,0.7363071676337425,0.33249236454765857
21,0.4441692754126221,-0.4896385167169952,1.0483323807661988,-0.2798513233382808,0.3654440411601482,-0.9511511224065868,1.07183417815835,-1.1125266878480011,1.6612452539705709,1.530926459469711,0.8065758349537706,-0.4608761859163296,1.953318153470006,-0.5804091884115288,-1.1361271547459526,0.3088224581068678
10,-0.4198721875238912,-0.4896385167169952,-0.599770896773018,-0.2798513233382808,-0.7402622069004932,1.282713681725651,-0.2706269833418844,0.5571598750882428,-0.8678825041257632,-0.9873294849630004,-0.3030941480165015,0.3964269940902554,-1.239747744444241,1.3242466681642504,0.7708315124166415,0.3062683346111502
50,-0.4187289718583732,0.383785300953989,-1.1586019265097969,-0.2798513233382808,-0.965722621544108,0.973562749010922,5.184108838826404e-16,0.6891223480045111,-0.5230014462035358,-1.1417508400461382,-1.6439453774389146,0.3898483790754975,-1.1598140429318595,1.4004329024272812,0.8623712129206954,0.2895103817146816
```

### Patterns in High MSE Data Points

- Data points exhibiting the highest Mean Squared Error (MSE) tend to show significant deviations across multiple features, indicating that the model struggles with predictions when input features deviate from the distribution typical of the training data.
- Specific features, such as RM, AGE, and LSTAT, may have values that are considerably different from those of other data points. This suggests these features may contribute to the model's challenges in accurately predicting these particular cases.

### Patterns in Low MSE Data Points

- In contrast, data points with the lowest MSE are generally more consistent and align closely with the mean or median values of the features in the dataset.
- For these cases, the model's predictions closely match the true labels, indicating strong performance on "average" scenarios where input data falls within the range seen during training.

### Impact of Outliers and Distribution

- High MSE data points could be classified as outliers or as instances where feature values are poorly represented in the training data. When encountering such atypical patterns, the model may fail to generalize effectively, resulting in greater prediction errors.
- Conversely, data points that are more centrally located within the feature distribution (represented by low MSE values) tend to incur lower prediction errors, suggesting the model has learned these patterns well.

### Possible Reasons

- **Non-linearity in Relationships:** Certain features may exhibit non-linear relationships with the target variable. A simple linear model, like logistic regression, might struggle to capture these complexities, leading to higher MSE in specific instances.
- **Feature Importance and Variability:** Features such as RM, LSTAT, or AGE could show higher variability in the dataset. If the model fails to account for this variability effectively, it may perform poorly on data points with extreme values of these features.
- **Model Overfitting or Underfitting:** An overfitting model may struggle to generalize to data points that differ from the training set, while an underfitting model may miss essential patterns in the data, both resulting in higher errors for certain cases.

Additionally, columns such as ZN and CHAS appear to increase MSE, indicating they are noisy, while RAD significantly influences MSE and is thus considered an important feature.

### Top 10 MSE Observations

- The highest MSE loss values are generally associated with the following:
  - **CRIM**: Varies widely, with values ranging from 29 to 32.
  - **NOX**: Has a negative impact on predictions, with most values around -0.27.
  - **RM**: Generally higher values lead to higher MSE, indicating a potential relationship.
  - **AGE**: Shows a mix of negative and positive impacts.

### Bottom 10 MSE Observations

- Lower MSE values are often linked to:
  - **CRIM**: Significantly lower values are observed.
  - **NOX**: Generally lower negative values than in the top 10.
  - **RM**: Values often hover around zero or are negative.
  - **LSTAT**: Typically has lower values, possibly correlating with reduced MSE.

The MSE analysis suggests that the model performs better on data points with feature values closer to those seen during training. However, when faced with outliers or unusual feature values, the model exhibits greater errors. This indicates a potential need to enhance the model's generalization capabilities, possibly through the use of more complex models or feature transformations, to reduce MSE across the dataset.

## AutoEncoders

### 4.1 AutoEncoder implementation from scratch

To create an autoencoder, we structure a neural network comprising input dimensions → hidden layers → latent dimensions → hidden layers (in reverse order) → output dimensions.

The initial section, which includes input dimensions, hidden layers, and latent dimensions, functions as the encoder. This part reduces the dimensionality of the data by identifying and preserving the key features, thereby minimizing the information loss that occurs during the reduction.

The subsequent section, transitioning from latent dimensions to hidden layers (in reverse) and finally to output dimensions, serves as the decoder. Its goal is to reconstruct the original data from the compressed representation.

This setup has been implemented using the previously developed MLPRegression model. The hidden layers utilize sigmoid activation functions, while the output layer employs a linear activation function, consistent with the nature of regression tasks.

The effectiveness of the autoencoder can be measured by evaluating the reconstruction loss.

### 4.2 Train the autoencoder

Optimal dimensions = 10 (From assignment 1)

### 4.3 AutoEncoder + KNN

```python
# PCA:
Reduced data shape: (114000, 10) # 10 Dimsensions
K: 24, Metric: Manhattan
Accuracy: 0.22736842105263158
Precision: 0.21654777187253502
Recall: 0.22674601907848696
F1 Score Macro: 0.22152958699048736
F1 Score Micro: 0.2058823529411765

```

```python
# AutoEncoder:
Validation Accuracy: 0.21219298245614035
Test:
Accuracy: 0.20175438596491227
Precision: 0.19013860522510592
Recall: 0.20239459941306567
F1 Score Macro: 0.19607526895956304
F1 Score Micro: 0.2213114754098361

Reconstruction Loss: 0.17
```

While PCA mathematically identifies key features through eigenvalues and eigenvectors of the covariance matrix, autoencoders learn these features independently.

Achieving a model that accurately captures latent features requires extensive hyperparameter tuning and experimentation, which has not been done.

### 4.4 MLP classification

On whole dataset:

KNN on whole:

```
K: 24, Metric: manhattan
Accuracy: 0.255
Precision: 0.24589041508480547
Recall: 0.25498415989593615
F1 Score Macro: 0.25035473569115724
F1 Score Micro: 0.2119815668202765

```

![image.png](Statistical%20Methods%20in%20Artificial%20Intelligence%2011b17aaec13280909daef4fe08a2d27a/image%2013.png)

```python
              precision    recall  f1-score   support

           0       0.10      0.10      0.10       103
           1       0.41      0.27      0.33       117
           2       0.10      0.01      0.02       100
           3       0.08      0.03      0.04       101
           4       0.27      0.25      0.26        96
           5       0.05      0.02      0.03       100
           6       0.53      0.46      0.49        92
           7       0.27      0.57      0.37        84
           8       0.14      0.04      0.06        98
           9       0.15      0.13      0.14        99
          10       0.46      0.23      0.30        97
          11       0.25      0.01      0.02        87
          12       0.21      0.22      0.21        97
          13       0.44      0.32      0.37       114
          14       0.38      0.36      0.37       104
          15       0.15      0.14      0.14        92
          16       0.50      0.62      0.55        94
          17       0.23      0.18      0.20        96
          18       0.94      0.80      0.86        93
          19       0.09      0.33      0.14        86
          20       0.12      0.15      0.13       110
          21       0.22      0.29      0.25       106
          22       0.26      0.49      0.34        96
          23       0.15      0.29      0.20        89
          24       0.40      0.56      0.47       110
          25       0.20      0.16      0.18       111
          26       0.29      0.30      0.29        99
          27       0.37      0.40      0.38        81
          28       0.08      0.02      0.03        92
          29       0.23      0.27      0.25       114
          30       0.00      0.00      0.00       116
          31       0.05      0.05      0.05        88
          32       0.06      0.01      0.02        98
          33       0.11      0.09      0.10        86
          34       0.27      0.03      0.05       106
          35       0.35      0.46      0.40       102
          36       0.12      0.04      0.06        93
          37       0.25      0.20      0.22        95
          38       0.18      0.12      0.14        94
          39       0.04      0.01      0.01       114
          40       0.27      0.54      0.36        98
          41       0.22      0.10      0.14       102
          42       0.73      0.83      0.78        98
          43       0.04      0.01      0.02       100
          44       0.13      0.10      0.11        91
          45       0.29      0.19      0.23       110
          46       0.36      0.30      0.32       104
          47       0.14      0.14      0.14       110
          48       0.30      0.47      0.37       104
          49       0.31      0.34      0.33        87
          50       0.24      0.23      0.24        95
          51       0.16      0.28      0.21        92
          52       0.72      0.77      0.74        92
          53       0.13      0.29      0.18        96
          54       0.48      0.37      0.42       115
          55       0.16      0.08      0.11       103
          56       0.00      0.00      0.00        95
          57       0.00      0.00      0.00        93
          58       0.13      0.09      0.10        94
          59       0.44      0.69      0.54       108
          60       0.44      0.43      0.43        94
          61       0.48      0.55      0.51       110
          62       0.07      0.02      0.03        93
          63       0.24      0.17      0.20       101
          64       0.31      0.37      0.34       106
          65       0.11      0.11      0.11        99
          66       0.63      0.63      0.63       118
          67       0.18      0.27      0.21        98
          68       0.18      0.09      0.12        74
          69       0.12      0.08      0.10        89
          70       0.18      0.44      0.26        99
          71       0.20      0.31      0.24       110
          72       0.27      0.28      0.28       108
          73       0.35      0.61      0.45       113
          74       0.14      0.24      0.18       102
          75       0.42      0.41      0.41       106
          76       0.39      0.42      0.40        89
          77       0.39      0.60      0.47       102
          78       0.30      0.58      0.39        96
          79       0.44      0.40      0.42        95
          80       0.28      0.58      0.38       105
          81       0.10      0.14      0.12        88
          82       0.27      0.40      0.32        90
          83       0.13      0.13      0.13        91
          84       0.19      0.16      0.17       112
          85       0.11      0.09      0.10        85
          86       0.12      0.10      0.11        93
          87       0.33      0.04      0.07       105
          88       0.11      0.16      0.14        85
          89       0.11      0.02      0.03       100
          90       0.20      0.35      0.25        88
          91       0.10      0.12      0.11       111
          92       0.31      0.25      0.28       111
          93       0.48      0.70      0.57       104
          94       0.27      0.46      0.34        90
          95       0.39      0.66      0.49        97
          96       0.29      0.40      0.34        95
          97       0.37      0.61      0.46        99
          98       0.23      0.10      0.14       102
          99       0.06      0.03      0.04       114
         100       0.17      0.36      0.23        91
         101       0.74      0.80      0.77       105
         102       0.00      0.00      0.00       108
         103       0.18      0.18      0.18       131
         104       0.09      0.01      0.02       112
         105       0.57      0.74      0.64       108
         106       0.12      0.02      0.03       104
         107       0.22      0.13      0.16       123
         108       0.74      0.68      0.71       103
         109       0.22      0.06      0.10       109
         110       0.24      0.31      0.27       105
         111       0.21      0.20      0.20       102
         112       0.16      0.34      0.22        99
         113       0.21      0.28      0.24        96

    accuracy                           0.28     11400
   macro avg       0.25      0.28      0.25     11400
weighted avg       0.26      0.28      0.25     11400

```

The accuracy shows an improvement compared to applying KNN on the entire dataset, though the change is not substantial since no hyperparameter tuning was performed. Nevertheless, the model still achieves better performance.The accuracy is better than the one when we do KNN on the entire dataset. The change is not significant due to no hyperparameter tuning done. However, it still performs better.

On 10 dimensions:

![image.png](Statistical%20Methods%20in%20Artificial%20Intelligence%2011b17aaec13280909daef4fe08a2d27a/image%2014.png)

```python
           precision    recall  f1-score   support

           0       0.08      0.12      0.09       103
           1       0.33      0.25      0.28       117
           2       0.00      0.00      0.00       100
           3       0.03      0.01      0.01       101
           4       0.32      0.24      0.28        96
           5       0.09      0.01      0.02       100
           6       0.43      0.45      0.44        92
           7       0.19      0.36      0.25        84
           8       0.14      0.01      0.02        98
           9       0.12      0.05      0.07        99
          10       0.36      0.19      0.24        97
          11       0.00      0.00      0.00        87
          12       0.19      0.09      0.12        97
          13       0.33      0.40      0.36       114
          14       0.21      0.27      0.24       104
          15       0.12      0.12      0.12        92
          16       0.47      0.57      0.52        94
          17       0.20      0.05      0.08        96
          18       0.91      0.80      0.85        93
          19       0.09      0.22      0.12        86
          20       0.05      0.08      0.07       110
          21       0.11      0.16      0.13       106
          22       0.20      0.28      0.23        96
          23       0.12      0.12      0.12        89
          24       0.35      0.40      0.37       110
          25       0.20      0.08      0.11       111
          26       0.42      0.20      0.27        99
          27       0.29      0.23      0.26        81
          28       0.05      0.03      0.04        92
          29       0.17      0.20      0.18       114
          30       0.00      0.00      0.00       116
          31       0.06      0.03      0.04        88
          32       0.00      0.00      0.00        98
          33       0.12      0.01      0.02        86
          34       0.00      0.00      0.00       106
          35       0.28      0.43      0.34       102
          36       0.24      0.04      0.07        93
          37       0.16      0.14      0.15        95
          38       0.19      0.05      0.08        94
          39       0.00      0.00      0.00       114
          40       0.17      0.54      0.26        98
          41       0.15      0.06      0.09       102
          42       0.61      0.78      0.68        98
          43       0.07      0.03      0.04       100
          44       0.08      0.12      0.09        91
          45       0.25      0.15      0.19       110
          46       0.26      0.20      0.23       104
          47       0.12      0.07      0.09       110
          48       0.12      0.17      0.14       104
          49       0.17      0.15      0.16        87
          50       0.18      0.14      0.15        95
          51       0.07      0.11      0.09        92
          52       0.50      0.62      0.55        92
          53       0.15      0.11      0.13        96
          54       0.45      0.24      0.32       115
          55       0.10      0.03      0.05       103
          56       0.00      0.00      0.00        95
          57       0.00      0.00      0.00        93
          58       0.12      0.06      0.08        94
          59       0.39      0.65      0.48       108
          60       0.29      0.44      0.35        94
          61       0.29      0.55      0.38       110
          62       0.20      0.08      0.11        93
          63       0.07      0.02      0.03       101
          64       0.29      0.24      0.26       106
          65       0.08      0.15      0.10        99
          66       0.42      0.54      0.47       118
          67       0.11      0.41      0.17        98
          68       0.11      0.04      0.06        74
          69       0.07      0.04      0.05        89
          70       0.15      0.26      0.19        99
          71       0.18      0.38      0.24       110
          72       0.23      0.21      0.22       108
          73       0.36      0.52      0.43       113
          74       0.15      0.29      0.20       102
          75       0.33      0.40      0.36       106
          76       0.27      0.37      0.31        89
          77       0.35      0.54      0.43       102
          78       0.26      0.41      0.32        96
          79       0.39      0.37      0.38        95
          80       0.12      0.39      0.18       105
          81       0.09      0.22      0.13        88
          82       0.24      0.22      0.23        90
          83       0.05      0.05      0.05        91
          84       0.13      0.12      0.13       112
          85       0.12      0.08      0.10        85
          86       0.10      0.09      0.09        93
          87       0.00      0.00      0.00       105
          88       0.07      0.09      0.08        85
          89       0.22      0.02      0.04       100
          90       0.20      0.31      0.25        88
          91       0.09      0.14      0.11       111
          92       0.15      0.05      0.07       111
          93       0.27      0.57      0.37       104
          94       0.14      0.23      0.18        90
          95       0.23      0.47      0.31        97
          96       0.43      0.34      0.38        95
          97       0.32      0.67      0.43        99
          98       0.14      0.09      0.11       102
          99       0.07      0.04      0.05       114
         100       0.13      0.34      0.18        91
         101       0.65      0.73      0.69       105
         102       0.00      0.00      0.00       108
         103       0.16      0.17      0.16       131
         104       0.00      0.00      0.00       112
         105       0.42      0.64      0.50       108
         106       0.00      0.00      0.00       104
         107       0.09      0.01      0.01       123
         108       0.40      0.57      0.47       103
         109       0.11      0.05      0.06       109
         110       0.23      0.31      0.26       105
         111       0.16      0.12      0.14       102
         112       0.15      0.27      0.20        99
         113       0.23      0.21      0.22        96

    accuracy                           0.22     11400
   macro avg       0.19      0.22      0.19     11400
weighted avg       0.19      0.22      0.19     11400

Test Accuracy: 0.2187
Test F1 Score: 0.1913
Test Precision: 0.1945
Test Recall: 0.2187
```

Running MLP on spotify dataset gives us an accuracy of 0.2187 which is the same compared to KNN accuracy which was 0.22736842105263158. The possible reason could be too simple neural net architecture which is unable to capture enough information to distinguish between 114 track genres. Performing hyperparameter tuning could give us better results.

MLP Used:

```
mlp = MLPClassifier(
        input_size=X_train.shape[1],
        output_size=len(np.unique(y_train)),
        hidden_layers=[64, 32, 16],
        activation="sigmoid",
        optimizer="sgd",
        learning_rate=0.01,
        batch_size=128,
        epochs=100,
        early_stopping=True,
        patience=5,
        wandb_log=False,
    )
```
